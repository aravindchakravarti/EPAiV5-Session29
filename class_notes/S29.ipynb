{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use KL Divergence loss on Knowledge Distillation Task. You can use any teacher and student model (prefer small models). You need to show that it works, and update README.md with proper logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch\n"
     ]
    }
   ],
   "source": [
    "print(\"Torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN Loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary Cross-Entropy Loss:\n",
      "Discriminator output (real): tensor([[0.4841],\n",
      "        [0.4257],\n",
      "        [0.5383],\n",
      "        [0.5834]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator output (fake): tensor([[0.5061],\n",
      "        [0.5181],\n",
      "        [0.5205],\n",
      "        [0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator loss (real): 0.6843972206115723\n",
      "Discriminator loss (fake): 0.7322050929069519\n",
      "Total Discriminator Loss: 1.416602373123169\n",
      "Generator Loss: 0.6558746695518494\n",
      "\n",
      "Least Squares Loss:\n",
      "Discriminator output (real): tensor([[0.4841],\n",
      "        [0.4257],\n",
      "        [0.5383],\n",
      "        [0.5834]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator output (fake): tensor([[0.5061],\n",
      "        [0.5181],\n",
      "        [0.5205],\n",
      "        [0.5316]], grad_fn=<SigmoidBackward0>)\n",
      "Discriminator loss (real): 0.12282979488372803\n",
      "Discriminator loss (fake): 0.1347564160823822\n",
      "Total Discriminator Loss: 0.25758621096611023\n",
      "Generator Loss: 0.11568927764892578\n"
     ]
    }
   ],
   "source": [
    "# GAN Loss\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set up device for computation\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "\n",
    "# Define Binary Cross-Entropy Loss for Vanilla GANs\n",
    "bce_loss = nn.BCELoss()\n",
    "\n",
    "# Define a sample batch size and the size of latent space\n",
    "batch_size = 4\n",
    "latent_dim = 100\n",
    "\n",
    "# Fake and real labels for the discriminator\n",
    "real_labels = torch.ones(batch_size, 1).to(device)  # Real labels (1s)\n",
    "fake_labels = torch.zeros(batch_size, 1).to(device)  # Fake labels (0s)\n",
    "\n",
    "# Inputs for Discriminator Loss\n",
    "# Real data (from the dataset)\n",
    "real_data = torch.randn(batch_size, 3, 64, 64).to(device)  # Simulated real images (e.g., 3 channels, 64x64)\n",
    "\n",
    "# Fake data (generated by the Generator)\n",
    "generator = nn.Sequential(\n",
    "    nn.Linear(latent_dim, 3 * 64 * 64),  # Fully connected layer to project latent space to image space\n",
    "    nn.Tanh()  # Tanh activation to output images with values between -1 and 1\n",
    ").to(device)\n",
    "\n",
    "latent_vectors = torch.randn(batch_size, latent_dim).to(device)  # Random noise (latent space)\n",
    "fake_data = generator(latent_vectors).view(batch_size, 3, 64, 64)  # Reshape to image dimensions\n",
    "\n",
    "# Discriminator (a simple model for this demonstration)\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(3 * 64 * 64, 128),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(128, 1),\n",
    "    nn.Sigmoid()  # Outputs probabilities between 0 and 1\n",
    ").to(device)\n",
    "\n",
    "# Forward pass for real and fake data through the discriminator\n",
    "real_output = discriminator(real_data)  # Discriminator output for real data\n",
    "fake_output = discriminator(fake_data.detach())  # Discriminator output for fake data\n",
    "\n",
    "# Calculate Discriminator Loss using Binary Cross-Entropy\n",
    "# Loss for real data (discriminator should classify it as real)\n",
    "loss_real = bce_loss(real_output, real_labels)\n",
    "\n",
    "# Loss for fake data (discriminator should classify it as fake)\n",
    "loss_fake = bce_loss(fake_output, fake_labels)\n",
    "\n",
    "d_loss = loss_real + loss_fake  # Total discriminator loss\n",
    "\n",
    "# Generator Loss (goal: fool the discriminator into classifying fake data as real)\n",
    "g_loss = bce_loss(discriminator(fake_data), real_labels)  # Use real_labels to simulate \"fooling\"\n",
    "\n",
    "# Print outputs for Binary Cross-Entropy Loss\n",
    "print(\"Binary Cross-Entropy Loss:\")\n",
    "print(\"Discriminator output (real):\", real_output)\n",
    "print(\"Discriminator output (fake):\", fake_output)\n",
    "print(\"Discriminator loss (real):\", loss_real.item())\n",
    "print(\"Discriminator loss (fake):\", loss_fake.item())\n",
    "print(\"Total Discriminator Loss:\", d_loss.item())\n",
    "print(\"Generator Loss:\", g_loss.item())\n",
    "\n",
    "# Least Squares Loss for LSGAN\n",
    "# Define real and fake labels for Least Squares Loss\n",
    "real_labels_ls = torch.ones(batch_size, 1).to(device)  # Real labels (target = 1)\n",
    "fake_labels_ls = torch.zeros(batch_size, 1).to(device)  # Fake labels (target = 0)\n",
    "\n",
    "# Calculate Discriminator Loss using Least Squares Loss\n",
    "loss_real_ls = 0.5 * torch.mean((real_output - real_labels_ls) ** 2)  # Loss for real data\n",
    "loss_fake_ls = 0.5 * torch.mean((fake_output - fake_labels_ls) ** 2)  # Loss for fake data\n",
    "\n",
    "d_loss_ls = loss_real_ls + loss_fake_ls  # Total discriminator loss\n",
    "\n",
    "# Generator Loss (goal: fool the discriminator into classifying fake data as real)\n",
    "g_loss_ls = 0.5 * torch.mean((discriminator(fake_data) - real_labels_ls) ** 2)\n",
    "\n",
    "# Print outputs for Least Squares Loss\n",
    "print(\"\\nLeast Squares Loss:\")\n",
    "print(\"Discriminator output (real):\", real_output)\n",
    "print(\"Discriminator output (fake):\", fake_output)\n",
    "print(\"Discriminator loss (real):\", loss_real_ls.item())\n",
    "print(\"Discriminator loss (fake):\", loss_fake_ls.item())\n",
    "print(\"Total Discriminator Loss:\", d_loss_ls.item())\n",
    "print(\"Generator Loss:\", g_loss_ls.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL (Kullback-Leibler) divergence loss is widely used in various contexts of deep neural networks (DNNs) and large language models (LLMs). Below are some key applications:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Knowledge Distillation**\n",
    "- **Purpose**: To transfer knowledge from a larger model (teacher) to a smaller model (student).\n",
    "- **Usage**: The student model is trained to minimize the KL divergence between the teacher's soft predictions (probability distribution over classes) and its own predictions.\n",
    "- **Equation**:\n",
    "  $$\n",
    "  \\text{KL}(P || Q) = \\\\sum P(x) \\\\log\\\\frac{P(x)}{Q(x)}\n",
    "  $$\n",
    "  where \\(P(x)\\) is the teacher's output distribution (softened with temperature), and \\(Q(x)\\) is the student's output.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2325\n",
      "Epoch 2, Loss: 0.2102\n",
      "Epoch 3, Loss: 0.1894\n",
      "Epoch 4, Loss: 0.1703\n",
      "Epoch 5, Loss: 0.1528\n",
      "Epoch 6, Loss: 0.1369\n",
      "Epoch 7, Loss: 0.1227\n",
      "Epoch 8, Loss: 0.1100\n",
      "Epoch 9, Loss: 0.0987\n",
      "Epoch 10, Loss: 0.0886\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple teacher model\n",
    "class TeacherModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TeacherModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # Input size 10, output size 5\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=1)  # Softmax for probability distribution\n",
    "\n",
    "# Define a simple student model\n",
    "class StudentModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StudentModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 5)  # Smaller capacity model with the same output size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=1)  # Softmax for probability distribution\n",
    "\n",
    "# Instantiate teacher and student models\n",
    "teacher = TeacherModel()\n",
    "student = StudentModel()\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "teacher.to(device)\n",
    "student.to(device)\n",
    "\n",
    "# Define a KL divergence loss\n",
    "kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_data = torch.randn(16, 10).to(device)  # Batch size 16, input size 10\n",
    "\n",
    "# Teacher model generates predictions\n",
    "with torch.no_grad():  # Teacher is pre-trained, so no gradient updates are needed\n",
    "    teacher_output = teacher(input_data)\n",
    "\n",
    "# Define optimizer for the student model\n",
    "optimizer = optim.Adam(student.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop for the student model\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    student_output = student(input_data)  # Student's predictions\n",
    "\n",
    "    # Compute KL divergence loss between teacher and student outputs\n",
    "    loss = kl_loss(torch.log(student_output), teacher_output)\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Variational Autoencoders (VAEs)**\n",
    "- **Purpose**: To learn a latent space representation of data.\n",
    "- **Usage**: In VAEs, KL divergence regularizes the latent space by ensuring that the approximate posterior distribution \\(q(z|x)\\) is close to the prior distribution \\(p(z)\\), typically a standard normal distribution \\(N(0, 1)\\).\n",
    "- **Loss Term**:\n",
    "  $$\n",
    "  \\\\text{KL}(q(z|x) || p(z)) = \\\\int q(z|x) \\\\log \\\\frac{q(z|x)}{p(z)} dz\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Reconstruction Loss: 177.1774, KL Loss: 7.4180, Total Loss: 184.5955\n",
      "Epoch 2, Reconstruction Loss: 177.2964, KL Loss: 6.0418, Total Loss: 183.3382\n",
      "Epoch 3, Reconstruction Loss: 178.3022, KL Loss: 4.9018, Total Loss: 183.2041\n",
      "Epoch 4, Reconstruction Loss: 173.0519, KL Loss: 3.9811, Total Loss: 177.0330\n",
      "Epoch 5, Reconstruction Loss: 175.4182, KL Loss: 3.2424, Total Loss: 178.6606\n",
      "Epoch 6, Reconstruction Loss: 173.0687, KL Loss: 2.6817, Total Loss: 175.7504\n",
      "Epoch 7, Reconstruction Loss: 171.7608, KL Loss: 2.2521, Total Loss: 174.0129\n",
      "Epoch 8, Reconstruction Loss: 167.9067, KL Loss: 1.9255, Total Loss: 169.8322\n",
      "Epoch 9, Reconstruction Loss: 168.9364, KL Loss: 1.6764, Total Loss: 170.6129\n",
      "Epoch 10, Reconstruction Loss: 168.7712, KL Loss: 1.4920, Total Loss: 170.2632\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 6)  # Input size 10, hidden size 6\n",
    "        self.fc_mu = nn.Linear(6, 3)  # Latent mean\n",
    "        self.fc_logvar = nn.Linear(6, 3)  # Latent log variance\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(hidden)\n",
    "        logvar = self.fc_logvar(hidden)\n",
    "        return mu, logvar\n",
    "\n",
    "# Define the decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 6)  # Latent size 3, hidden size 6\n",
    "        self.fc2 = nn.Linear(6, 10)  # Hidden size 6, output size 10\n",
    "\n",
    "    def forward(self, z):\n",
    "        hidden = torch.relu(self.fc1(z))\n",
    "        reconstructed = torch.sigmoid(self.fc2(hidden))\n",
    "        return reconstructed\n",
    "\n",
    "# Define the VAE model\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        # Reparameterization trick\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = mu + eps * std\n",
    "        reconstructed = self.decoder(z)\n",
    "        return reconstructed, mu, logvar\n",
    "\n",
    "# Instantiate the VAE model\n",
    "vae = VAE()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "vae.to(device)\n",
    "\n",
    "# Define optimizer and reconstruction loss\n",
    "optimizer = optim.Adam(vae.parameters(), lr=0.01)\n",
    "reconstruction_loss_fn = nn.MSELoss(reduction='sum')  # Reconstruction loss\n",
    "\n",
    "# Create a dummy input tensor\n",
    "input_data = torch.randn(16, 10).to(device)  # Batch size 16, input size 10\n",
    "\n",
    "# Training loop for the VAE\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    reconstructed, mu, logvar = vae(input_data)  # Forward pass\n",
    "\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = reconstruction_loss_fn(reconstructed, input_data)\n",
    "\n",
    "    # KL divergence loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    #kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Reconstruction Loss: {reconstruction_loss.item():.4f}, KL Loss: {kl_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Reinforcement Learning (Policy Optimization)**\n",
    "- **Purpose**: To ensure stability in policy updates during training.\n",
    "- **Usage**:\n",
    "  - In **Trust Region Policy Optimization (TRPO)** or **Proximal Policy Optimization (PPO)**, KL divergence is used to limit the step size when updating the policy to prevent drastic changes.\n",
    "  - KL divergence measures how different the new policy $$\\\\pi_{\\\\theta}(a|s)$$ is from the old policy $$\\\\pi_{\\\\text{old}}(a|s)$$.\n",
    "- **Loss Term**:\n",
    "  $$\n",
    "  \\\\text{KL}(\\\\pi_{\\\\text{old}} || \\\\pi_{\\\\theta})\n",
    "  $$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, KL Loss: 0.0141, Entropy: 0.6601, Total Loss: 0.0075\n",
      "Epoch 2, KL Loss: 0.0085, Entropy: 0.6697, Total Loss: 0.0018\n",
      "Epoch 3, KL Loss: 0.0053, Entropy: 0.6764, Total Loss: -0.0015\n",
      "Epoch 4, KL Loss: 0.0032, Entropy: 0.6805, Total Loss: -0.0036\n",
      "Epoch 5, KL Loss: 0.0020, Entropy: 0.6826, Total Loss: -0.0049\n",
      "Epoch 6, KL Loss: 0.0014, Entropy: 0.6835, Total Loss: -0.0054\n",
      "Epoch 7, KL Loss: 0.0014, Entropy: 0.6840, Total Loss: -0.0054\n",
      "Epoch 8, KL Loss: 0.0015, Entropy: 0.6845, Total Loss: -0.0053\n",
      "Epoch 9, KL Loss: 0.0016, Entropy: 0.6855, Total Loss: -0.0053\n",
      "Epoch 10, KL Loss: 0.0016, Entropy: 0.6867, Total Loss: -0.0053\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple policy network for reinforcement learning\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc2(torch.relu(self.fc1(x))), dim=-1)\n",
    "\n",
    "# Define a simple environment (e.g., 1D state space with discrete actions)\n",
    "state_dim = 4  # State size\n",
    "action_dim = 2  # Number of actions\n",
    "policy_old = PolicyNetwork(state_dim, action_dim)\n",
    "policy_new = PolicyNetwork(state_dim, action_dim)\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "policy_old.to(device)\n",
    "policy_new.to(device)\n",
    "\n",
    "# Define an optimizer for the new policy\n",
    "optimizer = optim.Adam(policy_new.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 16\n",
    "states = torch.randn(batch_size, state_dim).to(device)  # Random states\n",
    "old_probs = policy_old(states).detach()  # Probabilities from the old policy\n",
    "\n",
    "# Training loop for policy updates with KL divergence\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    # Get the new probabilities from the updated policy\n",
    "    new_probs = policy_new(states)\n",
    "\n",
    "    # Compute KL divergence between old and new policy probabilities\n",
    "    kl_loss = torch.sum(old_probs * torch.log(old_probs / new_probs), dim=1).mean()\n",
    "\n",
    "    # Add an optional entropy regularization term (to encourage exploration)\n",
    "    entropy = -torch.sum(new_probs * torch.log(new_probs + 1e-10), dim=1).mean()\n",
    "    total_loss = kl_loss - 0.01 * entropy  # Weight entropy with a small coefficient\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, KL Loss: {kl_loss.item():.4f}, Entropy: {entropy.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Language Modeling and Fine-Tuning**\n",
    "- **Purpose**: To align model outputs or guide behavior during training or fine-tuning.\n",
    "- **Usage**:\n",
    "  - **Supervised fine-tuning**: KL divergence is used when the target is a probability distribution, such as in token-level tasks (e.g., next-token prediction).\n",
    "  - **Aligning with human feedback**: In methods like Reinforcement Learning with Human Feedback (RLHF), KL divergence is used to constrain the fine-tuned policy to remain close to the pretrained model's behavior.\n",
    "  - **Loss Term**:\n",
    "    $$\n",
    "    \\\\mathcal{L}_{\\\\text{KL}} = \\\\beta \\\\cdot \\\\text{KL}(\\\\pi_{\\\\text{pretrained}} || \\\\pi_{\\\\text{fine-tuned}})\n",
    "    $$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, KL Loss: 0.0605, Entropy: 8.4844, Total Loss: -0.0244\n",
      "Epoch 2, KL Loss: 0.0540, Entropy: 8.4895, Total Loss: -0.0309\n",
      "Epoch 3, KL Loss: 0.0485, Entropy: 8.4938, Total Loss: -0.0365\n",
      "Epoch 4, KL Loss: 0.0438, Entropy: 8.4974, Total Loss: -0.0411\n",
      "Epoch 5, KL Loss: 0.0399, Entropy: 8.5004, Total Loss: -0.0451\n",
      "Epoch 6, KL Loss: 0.0367, Entropy: 8.5029, Total Loss: -0.0484\n",
      "Epoch 7, KL Loss: 0.0339, Entropy: 8.5049, Total Loss: -0.0511\n",
      "Epoch 8, KL Loss: 0.0317, Entropy: 8.5066, Total Loss: -0.0534\n",
      "Epoch 9, KL Loss: 0.0298, Entropy: 8.5080, Total Loss: -0.0552\n",
      "Epoch 10, KL Loss: 0.0283, Entropy: 8.5091, Total Loss: -0.0568\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Simple LLM-like policy network for RLHF\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc2(torch.relu(self.fc1(x))), dim=-1)\n",
    "\n",
    "# Define dimensions for input and output (e.g., token embeddings)\n",
    "input_dim = 128  # Input size (e.g., token embedding size)\n",
    "output_dim = 5000  # Output size (e.g., vocabulary size)\n",
    "\n",
    "# Instantiate pretrained and fine-tuned policies\n",
    "policy_pretrained = PolicyNetwork(input_dim, output_dim)\n",
    "policy_fine_tuned = PolicyNetwork(input_dim, output_dim)\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "policy_pretrained.to(device)\n",
    "policy_fine_tuned.to(device)\n",
    "\n",
    "# Define an optimizer for the fine-tuned policy\n",
    "optimizer = optim.Adam(policy_fine_tuned.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 16\n",
    "input_tokens = torch.randn(batch_size, input_dim).to(device)  # Random input token embeddings\n",
    "\n",
    "# Pretrained policy generates probabilities\n",
    "with torch.no_grad():\n",
    "    pretrained_probs = policy_pretrained(input_tokens).detach()  # Pretrained probabilities\n",
    "\n",
    "# Training loop for fine-tuning with KL divergence\n",
    "for epoch in range(10):  # Simple 10-epoch fine-tuning\n",
    "    # Get probabilities from the fine-tuned policy\n",
    "    fine_tuned_probs = policy_fine_tuned(input_tokens)\n",
    "\n",
    "    # Compute KL divergence loss between pretrained and fine-tuned policies\n",
    "    kl_loss = torch.sum(pretrained_probs * torch.log(pretrained_probs / fine_tuned_probs), dim=1).mean()\n",
    "\n",
    "    # Optionally, add entropy regularization to encourage exploration\n",
    "    entropy = -torch.sum(fine_tuned_probs * torch.log(fine_tuned_probs + 1e-10), dim=1).mean()\n",
    "    total_loss = kl_loss - 0.01 * entropy  # Adjust entropy weight as needed\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, KL Loss: {kl_loss.item():.4f}, Entropy: {entropy.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code demonstrates the use of KL divergence loss for fine-tuning a policy network in a Reinforcement Learning with Human Feedback (RLHF) setting. Here's what is happening step by step:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Two Policy Networks**\n",
    "- **`policy_pretrained`**:\n",
    "  - Represents the pretrained policy (e.g., a language model trained on general text data).\n",
    "  - It is used as a reference distribution and is not updated during training.\n",
    "\n",
    "- **`policy_fine_tuned`**:\n",
    "  - Represents the policy being fine-tuned to align with some feedback (e.g., human preferences).\n",
    "  - This network is updated during training to optimize its behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Input Data**\n",
    "- `input_tokens`:\n",
    "  - Simulates embeddings of tokens (e.g., input to a language model).\n",
    "  - Random embeddings are used here for simplicity, but in practice, they would be token representations from a dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Pretrained Probabilities**\n",
    "- The pretrained policy (`policy_pretrained`) generates a probability distribution over actions (or tokens in LLMs) given the input.\n",
    "- `pretrained_probs` are stored as the target distribution and detached from the computation graph (no gradient updates).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Fine-Tuned Probabilities**\n",
    "- The fine-tuned policy (`policy_fine_tuned`) also generates a probability distribution for the same input tokens.\n",
    "- These probabilities are compared with `pretrained_probs` using KL divergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. KL Divergence Loss**\n",
    "- KL divergence measures how much the fine-tuned policy's output (`fine_tuned_probs`) diverges from the pretrained policy's output (`pretrained_probs`).\n",
    "- Loss calculation:\n",
    "  $$\n",
    "  KL(P || Q) = \\\\sum P(x) \\\\log \\\\frac{P(x)}{Q(x)}\n",
    "  $$\n",
    "  - Here, \\(P(x)\\) is `pretrained_probs` and \\(Q(x)\\) is `fine_tuned_probs`.\n",
    "  - The goal is to minimize this divergence so the fine-tuned policy aligns closely with the pretrained policy.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Entropy Regularization**\n",
    "- To encourage exploration and avoid overconfidence in the fine-tuned policy:\n",
    "  - **Entropy** is computed as:\n",
    "    $$\n",
    "    -\\\\sum Q(x) \\\\log Q(x)\n",
    "    $$\n",
    "  - This term is subtracted from the total loss with a small weight (e.g., 0.01).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Optimization**\n",
    "- `total_loss` combines KL divergence loss and entropy regularization.\n",
    "- The gradients of `total_loss` are computed, and the optimizer updates the parameters of the fine-tuned policy.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Training**\n",
    "- For each epoch:\n",
    "  - `kl_loss`, `entropy`, and `total_loss` are computed and printed.\n",
    "  - The fine-tuned policy (`policy_fine_tuned`) gradually aligns with the pretrained policy while maintaining some exploratory behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose**\n",
    "This setup simulates how RLHF ensures that a fine-tuned language model aligns with a pretrained model's behavior while adjusting to human feedback. The KL divergence ensures the model does not deviate drastically, and entropy regularization maintains diversity in its outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Contrastive Learning**\n",
    "- **Purpose**: To encourage similarity between related representations and dissimilarity between unrelated ones.\n",
    "- **Usage**:\n",
    "  - In multi-modal learning (e.g., text and image alignment), KL divergence is used to align probability distributions of embeddings from different modalities.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 2, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 3, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 4, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 5, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 6, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 7, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 8, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 9, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n",
      "Epoch 10, Text KL Loss: nan, Image KL Loss: nan, Total Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define simple text and image encoders for contrastive learning\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(self.fc(x), p=2, dim=-1)  # L2 normalize\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(self.fc(x), p=2, dim=-1)  # L2 normalize\n",
    "\n",
    "# Define dimensions\n",
    "text_input_dim = 128  # Text embedding input size\n",
    "image_input_dim = 256  # Image embedding input size\n",
    "embed_dim = 64  # Common embedding dimension\n",
    "\n",
    "# Instantiate encoders\n",
    "text_encoder = TextEncoder(text_input_dim, embed_dim)\n",
    "image_encoder = ImageEncoder(image_input_dim, embed_dim)\n",
    "\n",
    "# Move models to the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "text_encoder.to(device)\n",
    "image_encoder.to(device)\n",
    "\n",
    "# Define an optimizer for both encoders\n",
    "optimizer = optim.Adam(list(text_encoder.parameters()) + list(image_encoder.parameters()), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 16\n",
    "text_embeddings = torch.randn(batch_size, text_input_dim).to(device)  # Simulated text embeddings\n",
    "image_embeddings = torch.randn(batch_size, image_input_dim).to(device)  # Simulated image embeddings\n",
    "\n",
    "# Training loop for contrastive learning\n",
    "for epoch in range(10):  # Simple 10-epoch training\n",
    "    # Encode text and image embeddings\n",
    "    text_features = text_encoder(text_embeddings)\n",
    "    image_features = image_encoder(image_embeddings)\n",
    "\n",
    "    # Compute similarity matrices\n",
    "    text_to_image_sim = torch.matmul(text_features, image_features.T)  # Text-to-image similarity\n",
    "    image_to_text_sim = torch.matmul(image_features, text_features.T)  # Image-to-text similarity\n",
    "\n",
    "    # Compute probability distributions\n",
    "    text_probs = nn.functional.softmax(text_to_image_sim, dim=1)  # Row-wise softmax\n",
    "    image_probs = nn.functional.softmax(image_to_text_sim, dim=1)  # Row-wise softmax\n",
    "\n",
    "    # Ground truth: diagonal should be the highest (self-similarity)\n",
    "    target_probs = torch.eye(batch_size).to(device)  # Identity matrix as ground truth\n",
    "\n",
    "    # Compute KL divergence loss\n",
    "    text_kl_loss = torch.sum(target_probs * torch.log(target_probs / text_probs), dim=1).mean()\n",
    "    image_kl_loss = torch.sum(target_probs * torch.log(target_probs / image_probs), dim=1).mean()\n",
    "    \n",
    "    # Total loss (average of text-to-image and image-to-text losses)\n",
    "    total_loss = (text_kl_loss + image_kl_loss) / 2\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Text KL Loss: {text_kl_loss.item():.4f}, Image KL Loss: {image_kl_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal Loss\n",
    "\n",
    "### **Focal Loss: Usage**\n",
    "\n",
    "Focal Loss is primarily designed to address the issue of class imbalance in classification tasks. It assigns more importance to hard-to-classify examples and reduces the weight for easily classified examples. Here's where it is commonly used:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Object Detection**\n",
    "- Widely used in **single-stage object detectors** like RetinaNet.\n",
    "- Helps the model focus on hard-to-detect objects (e.g., small, occluded, or overlapping objects) by reducing the loss contribution from well-classified examples (e.g., the background class in object detection).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Classification Loss: 0.3210, BBox Loss: 0.5274, Total Loss: 0.8485\n",
      "Epoch 2, Classification Loss: 0.3887, BBox Loss: 1.2665, Total Loss: 1.6552\n",
      "Epoch 3, Classification Loss: 0.4481, BBox Loss: 0.3934, Total Loss: 0.8415\n",
      "Epoch 4, Classification Loss: 0.2881, BBox Loss: 0.6357, Total Loss: 0.9238\n",
      "Epoch 5, Classification Loss: 0.1615, BBox Loss: 0.6546, Total Loss: 0.8161\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple object detection model (classification + bounding box regression)\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(16 * 16 * 16, num_classes)  # Classification head\n",
    "        self.bbox_regressor = nn.Linear(16 * 16 * 16, 4)  # Bounding box regression head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        class_logits = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        return class_logits, bbox_deltas\n",
    "\n",
    "# Define the Focal Loss for classification\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        \"\"\"\n",
    "        alpha: Balancing factor for class imbalance (e.g., positive vs negative classes).\n",
    "        gamma: Focusing parameter that emphasizes hard-to-classify examples.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha # balances the importance of different classes, \n",
    "        # particularly in presense of class imbalance\n",
    "        # α = 0.25 for positive class, and α = 0.75 for negative class\n",
    "        # (for binary classification)., it reduces the contribution of the \n",
    "        # dominant class to the loss and increases focus on minority\n",
    "        self.gamma = gamma # controls the focus on hard-to-classify images\n",
    "        # for well classified examples (p_t closing on 1), (1 - p_t)^gamma\n",
    "        # becomes small, reducing their contribution to total loss\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # Convert logits to probabilities using softmax\n",
    "        probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # One-hot encode the target labels\n",
    "        targets_one_hot = nn.functional.one_hot(targets, num_classes=logits.size(-1)).float()  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        # Extract probabilities for the true class (p_t)\n",
    "        pt = torch.sum(probs * targets_one_hot, dim=-1)  # Shape: (batch_size,)\n",
    "\n",
    "        # Compute the focal weight (alpha * (1 - p_t)^gamma)\n",
    "        focal_weight = self.alpha * (1 - pt) ** self.gamma\n",
    "\n",
    "        # Compute the focal loss (-focal_weight * log(p_t))\n",
    "        loss = -focal_weight * torch.log(pt + 1e-8)  # Adding a small constant to avoid log(0)\n",
    "\n",
    "        return loss.mean()  # Return the average loss\n",
    "\n",
    "# Instantiate the model and loss functions\n",
    "num_classes = 5\n",
    "model = ObjectDetectionModel(num_classes=num_classes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "classification_loss_fn = FocalLoss()\n",
    "bbox_loss_fn = nn.SmoothL1Loss()  # Smooth L1 loss for bounding box regression\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 8\n",
    "images = torch.randn(batch_size, 3, 32, 32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random images\n",
    "\n",
    "# Random class labels for the batch\n",
    "# Target classes are integers between 0 and num_classes-1\n",
    "# Example: [2, 0, 1, 3, ...] for a batch size of 8\n",
    "target_classes = torch.randint(0, num_classes, (batch_size,)).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Random bounding box coordinates for the batch\n",
    "# Example: [[x1, y1, x2, y2], ...] for each image\n",
    "# These are usually normalized to [0, 1] in real datasets\n",
    "target_bboxes = torch.randn(batch_size, 4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Training loop for object detection\n",
    "for epoch in range(5):  # Simple 5-epoch training\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass through the model\n",
    "    class_logits, bbox_deltas = model(images)\n",
    "\n",
    "    # Compute classification loss using Focal Loss\n",
    "    # The loss emphasizes hard-to-classify examples using the focal weight\n",
    "    cls_loss = classification_loss_fn(class_logits, target_classes)\n",
    "\n",
    "    # Compute bounding box regression loss using Smooth L1 Loss\n",
    "    # This loss penalizes large deviations between predicted and true bounding box coordinates\n",
    "    bbox_loss = bbox_loss_fn(bbox_deltas, target_bboxes)\n",
    "\n",
    "    # Total loss is the sum of classification and regression losses\n",
    "    total_loss = cls_loss + bbox_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    total_loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Classification Loss: {cls_loss.item():.4f}, BBox Loss: {bbox_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of Focal Loss Components\n",
    "\n",
    "Focal Loss is a modification of the standard cross-entropy loss to address class imbalance in classification tasks by down-weighting the loss for well-classified examples and focusing on hard-to-classify ones. Here's how it works:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `γ` (Gamma)**\n",
    "- **Purpose**: Controls the focus on hard-to-classify examples.\n",
    "- **Mechanism**:\n",
    "  - For well-classified examples (high probability for the true class, $$ p_t $$ near 1), the factor $$ (1 - p_t)^\\gamma $$ becomes small, reducing their contribution to the total loss.\n",
    "  - For hard-to-classify examples (low probability for the true class, $$ p_t $$ near 0), the factor $$ (1 - p_t)^\\gamma $$ remains large, increasing their contribution.\n",
    "- **Effect**: Higher $$ \\gamma $$ increases the focus on harder examples. Common values are $$ \\gamma = 2 $$ or $$ \\gamma = 3 $$.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `α` (Alpha)**\n",
    "- **Purpose**: Balances the importance of different classes, particularly in the presence of class imbalance.\n",
    "- **Mechanism**:\n",
    "  - If $$ \\alpha $$ is set to 0.25 for the positive class and 0.75 for the negative class (for binary classification), it reduces the contribution of the dominant class (negative) to the loss and increases the focus on the minority class (positive).\n",
    "- **Effect**: Helps balance the influence of underrepresented classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Focal Loss Formula**\n",
    "For a single example:\n",
    "$$\n",
    "\\text{FL}(p_t) = -\\alpha (1 - p_t)^\\gamma \\log(p_t)\n",
    "$$\n",
    "Where:\n",
    "- $$ p_t $$: Probability assigned to the true class.\n",
    "- $$ \\alpha $$: Balancing factor for class weights.\n",
    "- $$ \\gamma $$: Modulating factor for focusing on hard examples.\n",
    "\n",
    "---\n",
    "\n",
    "### **How it Works in the Code**\n",
    "1. **Logits to Probabilities**:\n",
    "   - `probs = torch.softmax(logits, dim=-1)` computes the probability distribution over classes.\n",
    "\n",
    "2. **True Class Probabilities**:\n",
    "   - `pt = torch.sum(probs * targets_one_hot, dim=-1)` extracts the predicted probabilities for the true class.\n",
    "\n",
    "3. **Focal Weight**:\n",
    "   - `focal_weight = self.alpha * (1 - pt) ** self.gamma` applies the modulating factor, emphasizing harder examples.\n",
    "\n",
    "4. **Final Loss**:\n",
    "   - `loss = -focal_weight * torch.log(pt + 1e-8)` computes the weighted log loss, where small $$ p_t $$ values contribute more due to the $$ (1 - p_t)^\\gamma $$ term.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary\n",
    "- **`γ` (Gamma)**: Focuses on hard examples by down-weighting easy examples.\n",
    "- **`α` (Alpha)**: Balances the contribution of different classes, addressing class imbalance.\n",
    "- Combined, they make Focal Loss effective in scenarios like object detection with highly imbalanced classes (e.g., background vs. objects). \n",
    "\n",
    "Let me know if you'd like further clarification or an expanded explanation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Imbalanced Datasets**\n",
    "- Used in classification tasks with **severe class imbalance**, such as:\n",
    "  - **Medical imaging**: Identifying rare diseases.\n",
    "  - **Fraud detection**: Classifying rare fraudulent transactions.\n",
    "  - **Anomaly detection**: Detecting rare events or behaviors.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Multi-label Classification**\n",
    "- Applied in scenarios where multiple labels can be assigned to an input, and there is an imbalance in the distribution of labels (e.g., detecting attributes in an image).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Semantic Segmentation**\n",
    "- Focuses on improving the classification of small or minority regions in segmentation tasks (e.g., segmenting rare object parts).\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Natural Language Processing (NLP)**\n",
    "- Useful in NLP tasks where certain classes are underrepresented, such as:\n",
    "  - Named Entity Recognition (NER) for rare entities.\n",
    "  - Sentiment analysis with imbalanced positive and negative reviews.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Multi-class Imbalance**\n",
    "- Tasks like image classification with a **long-tailed distribution** of classes benefit from Focal Loss, as it reduces the dominance of frequent classes.\n",
    "\n",
    "---\n",
    "\n",
    "Focal Loss is particularly effective when you want to focus on difficult-to-classify examples and mitigate the effects of imbalanced datasets, making it a versatile choice in various machine learning domains. Let me know if you'd like examples or a detailed mathematical explanation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IoU Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Classification Loss: 1.7452, IoU Loss: 1.0000, Total Loss: 2.7452\n",
      "Epoch 2, Classification Loss: 1.5240, IoU Loss: 1.0000, Total Loss: 2.5240\n",
      "Epoch 3, Classification Loss: 0.9614, IoU Loss: 1.0000, Total Loss: 1.9614\n",
      "Epoch 4, Classification Loss: 0.4466, IoU Loss: 1.0000, Total Loss: 1.4466\n",
      "Epoch 5, Classification Loss: 0.3174, IoU Loss: 1.0000, Total Loss: 1.3174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple object detection model (classification + bounding box regression)\n",
    "class ObjectDetectionModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ObjectDetectionModel, self).__init__()\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Linear(16 * 16 * 16, num_classes)  # Classification head\n",
    "        self.bbox_regressor = nn.Linear(16 * 16 * 16, 4)  # Bounding box regression head\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        features = features.view(features.size(0), -1)  # Flatten features\n",
    "        class_logits = self.classifier(features)\n",
    "        bbox_deltas = self.bbox_regressor(features)\n",
    "        return class_logits, bbox_deltas\n",
    "\n",
    "# Define IoU loss for bounding box regression\n",
    "class IoULoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IoULoss, self).__init__()\n",
    "\n",
    "    def forward(self, pred_boxes, target_boxes):\n",
    "        \"\"\"\n",
    "        pred_boxes: Predicted bounding boxes (batch_size, 4) [x1, y1, x2, y2]\n",
    "        target_boxes: Ground truth bounding boxes (batch_size, 4) [x1, y1, x2, y2]\n",
    "        \"\"\"\n",
    "        # Compute intersection\n",
    "        x1 = torch.max(pred_boxes[:, 0], target_boxes[:, 0])\n",
    "        y1 = torch.max(pred_boxes[:, 1], target_boxes[:, 1])\n",
    "        x2 = torch.min(pred_boxes[:, 2], target_boxes[:, 2])\n",
    "        y2 = torch.min(pred_boxes[:, 3], target_boxes[:, 3])\n",
    "\n",
    "        intersection = torch.clamp(x2 - x1, min=0) * torch.clamp(y2 - y1, min=0)\n",
    "\n",
    "        # Compute areas of predicted and target boxes\n",
    "        pred_area = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])\n",
    "        target_area = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])\n",
    "\n",
    "        # Compute union\n",
    "        union = pred_area + target_area - intersection\n",
    "\n",
    "        # Compute IoU\n",
    "        iou = intersection / (union + 1e-6)  # Add a small constant to avoid division by zero\n",
    "\n",
    "        # IoU loss\n",
    "        loss = 1 - iou.mean()  # Minimize 1 - IoU\n",
    "        return loss\n",
    "\n",
    "# Instantiate the model and loss functions\n",
    "num_classes = 5\n",
    "model = ObjectDetectionModel(num_classes=num_classes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "classification_loss_fn = nn.CrossEntropyLoss()  # Cross-entropy for classification\n",
    "bbox_loss_fn = IoULoss()  # IoU loss for bounding box regression\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 8\n",
    "images = torch.randn(batch_size, 3, 32, 32).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random images\n",
    "\n",
    "# Random class labels for the batch\n",
    "target_classes = torch.randint(0, num_classes, (batch_size,)).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random class labels\n",
    "\n",
    "# Random bounding box coordinates for the batch\n",
    "target_bboxes = torch.randn(batch_size, 4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Random ground truth boxes\n",
    "pred_bboxes = torch.randn(batch_size, 4).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Predicted boxes\n",
    "\n",
    "# Training loop for object detection\n",
    "for epoch in range(5):  # Simple 5-epoch training\n",
    "    model.train()\n",
    "\n",
    "    # Forward pass through the model\n",
    "    class_logits, bbox_deltas = model(images)\n",
    "\n",
    "    # Compute classification loss\n",
    "    cls_loss = classification_loss_fn(class_logits, target_classes)\n",
    "\n",
    "    # Compute IoU loss for bounding boxes\n",
    "    bbox_loss = bbox_loss_fn(bbox_deltas, target_bboxes)\n",
    "\n",
    "    # Total loss is the sum of classification and IoU losses\n",
    "    total_loss = cls_loss + bbox_loss\n",
    "\n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    total_loss.backward()  # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch {epoch + 1}, Classification Loss: {cls_loss.item():.4f}, IoU Loss: {bbox_loss.item():.4f}, Total Loss: {total_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Perceptual Loss: Where It Is Used**\n",
    "\n",
    "Perceptual loss measures the perceptual similarity between images rather than pixel-wise differences, focusing on how humans perceive visual differences. It relies on pre-trained deep neural networks, such as VGG, to compare features from intermediate layers. Here are its primary use cases:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Image Super-Resolution**\n",
    "- Perceptual loss is used to train models that generate high-resolution images from low-resolution inputs.\n",
    "- It ensures the output image is visually similar to the ground truth in terms of texture and structure, beyond pixel-level accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Image-to-Image Translation**\n",
    "- Applied in tasks like style transfer, domain adaptation, and image colorization.\n",
    "- Perceptual loss ensures that the transformed image retains the content structure of the input while matching the desired target style.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Generative Adversarial Networks (GANs)**\n",
    "- In tasks like image synthesis and inpainting, perceptual loss helps improve the quality of the generated image by aligning it with human perception.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Neural Style Transfer**\n",
    "- Used to minimize differences in content and style between the input and target images.\n",
    "- Content loss focuses on structural similarity, while style loss ensures stylistic consistency.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Video Frame Prediction**\n",
    "- Ensures temporal consistency in videos by preserving perceptual quality across consecutive frames.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. 3D Reconstruction**\n",
    "- Helps in aligning 3D models or reconstructed objects with 2D image views, improving texture and geometry quality.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Image Denoising and Deblurring**\n",
    "- Used to improve perceptual quality by focusing on restoring image details as seen by humans.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Face Reconstruction and Alignment**\n",
    "- Ensures facial landmarks and features are preserved during tasks like face swapping, alignment, or super-resolution.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptual Loss: 0.0274\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple perceptual loss using a pre-trained VGG-like network\n",
    "class PerceptualLoss(nn.Module):\n",
    "    def __init__(self, feature_extractor):\n",
    "        super(PerceptualLoss, self).__init__()\n",
    "        self.feature_extractor = feature_extractor  # Pre-trained feature extractor\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False  # Freeze the feature extractor\n",
    "\n",
    "    def forward(self, input_image, target_image):\n",
    "        \"\"\"\n",
    "        input_image: Generated image (batch_size, channels, height, width)\n",
    "        target_image: Ground truth image (batch_size, channels, height, width)\n",
    "        \"\"\"\n",
    "        input_features = self.feature_extractor(input_image)\n",
    "        target_features = self.feature_extractor(target_image)\n",
    "        loss = nn.functional.mse_loss(input_features, target_features)  # MSE on feature maps\n",
    "        return loss\n",
    "\n",
    "# Define a simple feature extractor (e.g., part of a VGG-like network)\n",
    "class SimpleFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleFeatureExtractor, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "# Instantiate the feature extractor and perceptual loss\n",
    "feature_extractor = SimpleFeatureExtractor().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "perceptual_loss_fn = PerceptualLoss(feature_extractor)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "input_images = torch.randn(batch_size, 3, 64, 64).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Generated images\n",
    "target_images = torch.randn(batch_size, 3, 64, 64).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Ground truth images\n",
    "\n",
    "# Compute perceptual loss\n",
    "loss = perceptual_loss_fn(input_images, target_images)\n",
    "print(f\"Perceptual Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC Loss\n",
    "\n",
    "### **CTC Loss: Overview**\n",
    "\n",
    "**Connectionist Temporal Classification (CTC) Loss** is designed for sequence prediction tasks where:\n",
    "1. The alignment between input and output sequences is unknown.\n",
    "2. The input and output sequence lengths may differ.\n",
    "\n",
    "It is commonly used in applications where outputs have varying lengths, such as speech recognition, handwriting recognition, and OCR (optical character recognition).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Features**\n",
    "1. **Alignment-Free**: \n",
    "   - CTC automatically handles the alignment of input and output sequences.\n",
    "   - For example, in speech recognition, the model predicts a probability distribution over characters for each time step of the audio, and CTC determines the best alignment.\n",
    "\n",
    "2. **Blank Tokens**: \n",
    "   - CTC introduces a special \"blank\" token to handle varying sequence lengths.\n",
    "   - Blank tokens are used to fill gaps between output tokens when no explicit output is required.\n",
    "\n",
    "3. **Collapse Repeated Tokens**: \n",
    "   - CTC maps repeated predictions (e.g., \"hhheeelllooo\") and blanks to the target sequence (\"hello\").\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Speech Recognition**:\n",
    "   - Used in end-to-end speech models like DeepSpeech.\n",
    "   - Converts acoustic feature sequences into text without requiring pre-aligned transcriptions.\n",
    "   \n",
    "2. **Handwriting Recognition**:\n",
    "   - Maps handwritten strokes to characters or words.\n",
    "\n",
    "3. **Optical Character Recognition (OCR)**:\n",
    "   - Used in systems where text is extracted from images without strict alignment between input pixels and output characters.\n",
    "\n",
    "4. **Sign Language Recognition**:\n",
    "   - Predicts the sequence of words or letters corresponding to gestures in sign language.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "1. The model outputs a probability distribution over the set of possible characters (including the blank token) for each time step.\n",
    "2. CTC computes the total probability of all valid alignments that map to the target sequence.\n",
    "3. The loss minimizes the negative log probability of the correct output sequence given the input sequence.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTC Loss: 4.5347\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple Transformer-based model for sequence prediction\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_classes, num_heads=2, num_layers=2):\n",
    "        super(SimpleTransformer, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, model_dim)\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(model_dim, num_classes + 1)  # +1 for the blank token\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # Project input to model dimension\n",
    "        x = self.transformer(x)  # Pass through transformer layers\n",
    "        x = self.fc(x)  # Project to class probabilities (including blank)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 13  # Example input dimension (e.g., MFCCs for speech)\n",
    "model_dim = 64  # Model dimension\n",
    "num_classes = 10  # Number of output classes\n",
    "\n",
    "model = SimpleTransformer(input_dim, model_dim, num_classes).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Define the CTC Loss\n",
    "ctc_loss_fn = nn.CTCLoss(blank=num_classes, reduction='mean', zero_infinity=True)\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "sequence_length = 15  # Length of input sequences\n",
    "output_length = 5  # Length of target sequences\n",
    "\n",
    "# Inputs (e.g., MFCC features for speech)\n",
    "inputs = torch.randn(sequence_length, batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Targets (e.g., token indices)\n",
    "targets = torch.randint(0, num_classes, (batch_size * output_length,)).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Input and target lengths\n",
    "input_lengths = torch.full((batch_size,), sequence_length, dtype=torch.long).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "target_lengths = torch.full((batch_size,), output_length, dtype=torch.long).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Forward pass\n",
    "logits = model(inputs)  # Shape: (sequence_length, batch_size, num_classes + 1)\n",
    "log_probs = nn.functional.log_softmax(logits, dim=-1)  # Log probabilities for CTC\n",
    "\n",
    "# Compute CTC Loss\n",
    "loss = ctc_loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "print(f\"CTC Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet loss\n",
    "\n",
    "### **Triplet Loss: Overview**\n",
    "\n",
    "**Triplet Loss** is a type of loss function used for **metric learning**. It ensures that embeddings of similar items are closer together while embeddings of dissimilar items are farther apart in the feature space.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Works**\n",
    "Triplet loss operates on **triplets of data**:\n",
    "1. **Anchor**: The reference data point.\n",
    "2. **Positive**: A data point similar to the anchor.\n",
    "3. **Negative**: A data point dissimilar to the anchor.\n",
    "\n",
    "The loss is designed to minimize the distance between the **anchor** and **positive** embeddings while maximizing the distance between the **anchor** and **negative** embeddings.\n",
    "\n",
    "The mathematical formulation:\n",
    "\\[\n",
    "\\mathcal{L}_{\\text{triplet}} = \\max \\left( d(a, p) - d(a, n) + \\text{margin}, 0 \\\\right)\n",
    "\\]\n",
    "Where:\n",
    "- \\( a, p, n \\): Anchor, Positive, and Negative embeddings.\n",
    "- \\( d(x, y) \\): Distance metric (usually L2 or cosine distance).\n",
    "- **Margin**: A predefined constant that defines the minimum required separation between \\( d(a, p) \\) and \\( d(a, n) \\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Face Recognition**:\n",
    "   - Used in models like FaceNet to ensure embeddings of the same person are closer than embeddings of different people.\n",
    "2. **Image Retrieval**:\n",
    "   - Ensures that visually similar images are closer in the embedding space.\n",
    "3. **Speaker Verification**:\n",
    "   - Verifies if two audio samples belong to the same speaker.\n",
    "4. **Signature Verification**:\n",
    "   - Determines whether two signatures belong to the same person.\n",
    "5. **Product Recommendation**:\n",
    "   - Ensures embeddings of similar products are closer together in recommendation systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triplet Loss: 1.2950\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple embedding model for triplet loss\n",
    "class SimpleEmbeddingModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(SimpleEmbeddingModel, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)  # Project to embedding space\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.normalize(self.fc(x), p=2, dim=-1)  # Normalize embeddings\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 10  # Input feature dimension\n",
    "embedding_dim = 5  # Embedding space dimension\n",
    "model = SimpleEmbeddingModel(input_dim, embedding_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Define Triplet Loss\n",
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)  # L2 distance with margin 1.0\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "anchor = torch.randn(batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Anchor samples\n",
    "positive = torch.randn(batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Positive samples\n",
    "negative = torch.randn(batch_size, input_dim).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))  # Negative samples\n",
    "\n",
    "# Forward pass to compute embeddings\n",
    "anchor_embed = model(anchor)\n",
    "positive_embed = model(positive)\n",
    "negative_embed = model(negative)\n",
    "\n",
    "# Compute Triplet Loss\n",
    "loss = triplet_loss_fn(anchor_embed, positive_embed, negative_embed)\n",
    "print(f\"Triplet Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICE Loss\n",
    "\n",
    "### **Dice Loss: Overview**\n",
    "\n",
    "**Dice Loss** is a loss function designed for **binary and multi-class segmentation tasks**. It measures the overlap between the predicted segmentation and the ground truth, focusing on minimizing differences in areas of the objects being segmented.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Definition**\n",
    "The **Dice Coefficient** (or Dice Similarity Coefficient, DSC) measures the similarity between two sets (predicted and ground truth masks):\n",
    "$$\n",
    "\\text{Dice Coefficient} = \\frac{2 |A \\cap B|}{|A| + |B|}\n",
    "$$\n",
    "Where:\n",
    "- \\(A\\) = Predicted mask.\n",
    "- \\(B\\) = Ground truth mask.\n",
    "\n",
    "The **Dice Loss** is defined as:\n",
    "$$\n",
    "\\text{Dice Loss} = 1 - \\text{Dice Coefficient}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Image Segmentation**:\n",
    "   - Used in tasks such as medical imaging (e.g., tumor segmentation in MRI scans).\n",
    "   - Ensures better overlap between predicted and actual segmentations.\n",
    "\n",
    "2. **Imbalanced Data**:\n",
    "   - Particularly effective when dealing with highly imbalanced classes (e.g., small objects in large images).\n",
    "   - Penalizes false positives and false negatives equally.\n",
    "\n",
    "3. **Binary and Multi-Class Segmentation**:\n",
    "   - Can be extended for multi-class segmentation tasks by averaging Dice Loss across all classes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages**\n",
    "1. Focuses on overlap, which is more relevant for segmentation tasks than simple pixel-wise losses (like cross-entropy).\n",
    "2. Handles class imbalance better, as it normalizes by the total size of the masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dice Loss: 0.6005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define Dice Loss\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        \"\"\"\n",
    "        preds: Predicted segmentation map (batch_size, num_classes, height, width)\n",
    "        targets: Ground truth segmentation map (batch_size, num_classes, height, width)\n",
    "        \"\"\"\n",
    "        smooth = 1.0  # To avoid division by zero\n",
    "        preds = preds.softmax(dim=1)  # Apply softmax to get probabilities\n",
    "\n",
    "        # Flatten predictions and targets for Dice computation\n",
    "        preds_flat = preds.view(preds.size(0), preds.size(1), -1)  # (batch_size, num_classes, -1)\n",
    "        targets_flat = targets.view(targets.size(0), targets.size(1), -1)  # (batch_size, num_classes, -1)\n",
    "\n",
    "        # Compute intersection and union\n",
    "        intersection = (preds_flat * targets_flat).sum(dim=2)  # Intersection per class\n",
    "        union = preds_flat.sum(dim=2) + targets_flat.sum(dim=2)  # Union per class\n",
    "\n",
    "        # Dice coefficient\n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "\n",
    "        # Average over batch and classes\n",
    "        dice_loss = 1 - dice.mean()\n",
    "        return dice_loss\n",
    "\n",
    "# Dummy data for demonstration\n",
    "batch_size = 4\n",
    "num_classes = 3\n",
    "height, width = 64, 64\n",
    "\n",
    "# Predictions (logits) from a model\n",
    "preds = torch.randn(batch_size, num_classes, height, width).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Ground truth segmentation maps (one-hot encoded)\n",
    "targets = torch.randint(0, 2, (batch_size, num_classes, height, width)).float().to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "# Instantiate Dice Loss\n",
    "dice_loss_fn = DiceLoss()\n",
    "\n",
    "# Compute Dice Loss\n",
    "loss = dice_loss_fn(preds, targets)\n",
    "print(f\"Dice Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance and Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28092\\836188694.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_float32_matmul_precision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'high'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGPT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mGPTConfig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28092\\836188694.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# weight initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \"\"\"\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \"\"\"\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \"\"\"\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \"\"\"\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \"\"\"\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    883\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28092\\836188694.py\u001b[0m in \u001b[0;36m_init_weights\u001b[1;34m(self, module)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'NANGPT_SCALE_INIT'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mstd\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36mnormal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrunc_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mi:\\Installs\\lib\\site-packages\\torch\\nn\\init.py\u001b[0m in \u001b[0;36m_no_grad_normal_\u001b[1;34m(tensor, mean, std)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_no_grad_normal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Base Model\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
    "        # regularization\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
    "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        # att = F.softmax(att, dim=-1)\n",
    "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "\n",
    "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True) # Flash attention\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "        # output projection\n",
    "        y = self.c_proj(y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.gelu    = nn.GELU(approximate='tanh')\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024 # max sequence length\n",
    "    vocab_size: int = 50304 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
    "    n_layer: int = 12 # number of layers\n",
    "    n_head: int = 12 # number of heads\n",
    "    n_embd: int = 768 # embedding dimension\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # weight sharing\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # weight initialization\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            std = 0.02\n",
    "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
    "                std *= (2 * self.config.n_layer) ** -0.5\n",
    "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx is of shape (B, T)\n",
    "        B, T = idx.size()\n",
    "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
    "        # forward the token and posisition embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
    "        x = tok_emb + pos_emb\n",
    "        # forward the blocks of the transformer\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        # forward the final layernorm and the classifier\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "\n",
    "# model = GPT.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# SEED\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(1337)\n",
    "\n",
    "# STOP\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        # at init load tokens from disk and store them in memory\n",
    "        with open('input.txt', 'r') as f:\n",
    "            text = f.read()\n",
    "        enc = tiktoken.get_encoding('gpt2') \n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T)} batches')\n",
    "\n",
    "        # state\n",
    "        self.current_position = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position: self.current_position + B * T + 1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B*T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y\n",
    "\n",
    "# CHANGES IN CURRENT CODE\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B = 16, T = 1024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Memory and Compute Requirement notebook\n",
    "\n",
    "This notebook is a simple tool to estimate the memory and compute requirements for the solution of the problem. The notebook is divided into two sections: the first one is dedicated to the memory requirements, while the second one is dedicated to the compute requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings               38,597,376      31.24%\n",
      "Attention                      28,311,552      22.91%\n",
      "MLP                            56,623,104      45.83%\n",
      "Norm                               19,200       0.02%\n",
      "Output                                  0       0.00%\n",
      "Total parameters: 123,551,232\n"
     ]
    }
   ],
   "source": [
    "# Some working statistics\n",
    "\n",
    "# Model\n",
    "vocabulary_size = 50257\n",
    "embedding_dimensions = 768\n",
    "num_attention_heads = 12\n",
    "num_hidden_layers = 12\n",
    "feed_forward_ratio = 4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "parameter_counts = {\n",
    "    \"Token Embeddings\": vocabulary_size * embedding_dimensions,\n",
    "    \"Attention\": (\n",
    "        embedding_dimensions * 3 * embedding_dimensions + embedding_dimensions**2\n",
    "    ) * num_hidden_layers,\n",
    "    \"MLP\": (\n",
    "        embedding_dimensions * feed_forward_ratio * embedding_dimensions * 2\n",
    "    ) * num_hidden_layers,\n",
    "    \"Norm\": embedding_dimensions * 2 * num_hidden_layers + embedding_dimensions,\n",
    "    \"Output\": 0, # We share the embedding weights\n",
    "}\n",
    "\n",
    "plt.bar(parameter_counts.keys(), parameter_counts.values())\n",
    "\n",
    "plt.title(\"Model Parameters\")\n",
    "plt.ylabel(\"# of Parameters\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_parameter_count = sum(parameter_counts.values())\n",
    "\n",
    "for name, count in parameter_counts.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_parameter_count * 100:10.2f}%\")\n",
    "\n",
    "\n",
    "print(f\"Total parameters: {total_parameter_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of the \"shape\" of our neural network we'll look at the ratio of embeding dimensions to number of layers. Generally, a aspect ratio betweem 50 and 100 is considered optimal according to certain scaling laws (Kaplan et al., 2020)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network has an aspect ratio of 64.00\n"
     ]
    }
   ],
   "source": [
    "aspect_ratio = embedding_dimensions / num_hidden_layers\n",
    "\n",
    "print(f\"Network has an aspect ratio of {aspect_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same abalysis for the ratio of embedding dimensions to the number of attention heads. A ratio between 20 and 80 is considered optimal according to the same paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heads ratio is 64.00\n"
     ]
    }
   ],
   "source": [
    "heads_ratio = embedding_dimensions / num_attention_heads\n",
    "\n",
    "print(f\"Heads ratio is {heads_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the size of the model in memory and on disk. Note that this does not include any intermediate variables that get memorized during training such as activations, gradients, optimizer state, and temporary buffers. Actual memory consumption will likely be much higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total gigabytes: 0.49G\n"
     ]
    }
   ],
   "source": [
    "bytes_per_parameter = 32 // 8  # Assuming 32-bit floating point\n",
    "\n",
    "total_bytes = total_parameter_count * bytes_per_parameter\n",
    "\n",
    "total_gigabytes = total_bytes / 1e9\n",
    "\n",
    "print(f\"Total gigabytes: {total_gigabytes:,.2f}G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate the optimal number of training tokens using the Chinchilla scaling laws given the number of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal training tokens: 2,471,024,640\n",
      "Epochs required: 2,357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_training_tokens = 20 * total_parameter_count\n",
    "samples_per_epoch = 512\n",
    "tokens_per_sample = 2048\n",
    "num_epochs_required = round(\n",
    "    num_training_tokens / (samples_per_epoch * tokens_per_sample)\n",
    ")\n",
    "print(f\"Optimal training tokens: {num_training_tokens:,}\")\n",
    "\n",
    "print(f\"Epochs required: {num_epochs_required:,}\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the maximum number of floating point operations (FLOPs) required to perform a full forward pass of the nwtwork on a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention                 270,582,939,648      40.96%\n",
      "MLP                       231,928,455,168      35.11%\n",
      "RMS Norm                           69,888       0.00%\n",
      "Output Layer              158,094,852,096      23.93%\n",
      "\n",
      "\n",
      "Total forward FLOPs: 660,606,316,800\n"
     ]
    }
   ],
   "source": [
    "ops_per_matmul = 2 # Multiply + accumulate (MAC)\n",
    "# ops_per_activation = 5 # Assuming SiLU\n",
    "ops_per_activation = 6 # Assuming GeLU\n",
    "ops_per_rms_norm = 7 # y = x / sqrt(sum(x^2) / n + epsilon) * gamma\n",
    "head_dimensions = embedding_dimensions // num_attention_heads\n",
    "\n",
    "# K, Q, V projections\n",
    "attention = (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * (embedding_dimensions * 3 * embedding_dimensions)\n",
    ")\n",
    "\n",
    "# Attention logits\n",
    "attention += (\n",
    "    ops_per_matmul * tokens_per_sample * tokens_per_sample * embedding_dimensions\n",
    ")\n",
    "\n",
    "# Reductions\n",
    "attention += (\n",
    "    ops_per_matmul\n",
    "    * num_attention_heads\n",
    "    * (tokens_per_sample * tokens_per_sample * head_dimensions)\n",
    ")\n",
    "\n",
    "# Output projection\n",
    "attention += ops_per_matmul * tokens_per_sample * embedding_dimensions**2\n",
    "\n",
    "attention *= num_hidden_layers\n",
    "\n",
    "# Linear transformations\n",
    "mlp = (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * (embedding_dimensions * (4 * embedding_dimensions))\n",
    ")\n",
    "mlp += (\n",
    "    ops_per_matmul\n",
    "    * tokens_per_sample\n",
    "    * ((4 * embedding_dimensions) * embedding_dimensions)\n",
    ")\n",
    "\n",
    "# Non-linear activations\n",
    "mlp += ops_per_activation * (4 * embedding_dimensions)\n",
    "\n",
    "mlp *= num_hidden_layers\n",
    "\n",
    "rms_norm = ops_per_rms_norm * embedding_dimensions * (num_hidden_layers + 1)\n",
    "\n",
    "output_layer = (\n",
    "    ops_per_matmul * tokens_per_sample * embedding_dimensions * vocabulary_size\n",
    ")\n",
    "\n",
    "flops = {\n",
    "    \"Attention\": attention,\n",
    "    \"MLP\": mlp,\n",
    "    \"RMS Norm\": rms_norm,\n",
    "    \"Output Layer\": output_layer,\n",
    "}\n",
    "\n",
    "plt.bar(flops.keys(), flops.values())\n",
    "\n",
    "plt.title(\"Model Operations\")\n",
    "plt.ylabel(\"# of FLOPs\")\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "total_forward_flops = sum(flops.values())\n",
    "\n",
    "for name, count in flops.items():\n",
    "    print(f\"{name:20s} {count:20,d} {count / total_forward_flops * 100:10.2f}%\")\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Total forward FLOPs: {total_forward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll estimate the number of FLOPs for the backward pass. For this we use a simple heuristic of 2x the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total backward FLOPs: 1,321,212,633,600\n"
     ]
    }
   ],
   "source": [
    "total_backward_flops = 2 * total_forward_flops\n",
    "\n",
    "print(f\"Total backward FLOPs: {total_backward_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll do the same for the total FLOPs per roundtrip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total roundtrip FLOPs: 1,981,818,950,400\n"
     ]
    }
   ],
   "source": [
    "total_roundtrip_flops = total_forward_flops + total_backward_flops\n",
    "\n",
    "print(f\"Total roundtrip FLOPs: {total_roundtrip_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's estimate the number of FLOPs using the method in the PaLM paper by Chowdhery, et al. Then, we'll compare the PaLM estimation with our own as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total PaLM FLOPs: 1,982,054,006,784\n"
     ]
    }
   ],
   "source": [
    "palm_flops_per_token = (\n",
    "    6 * total_parameter_count\n",
    "    + 12 * num_hidden_layers * num_attention_heads * head_dimensions * tokens_per_sample\n",
    ")\n",
    "\n",
    "total_palm_flops = palm_flops_per_token * tokens_per_sample\n",
    "\n",
    "print(f\"Total PaLM FLOPs: {total_palm_flops:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two estimates are pretty close so let's proceed.\n",
    "\n",
    "Finally, let's estimate how long it would take to train over the optimal number of tokens given some common Nvidia Ampere generation GPU hardware configurations. Note that these results shown here are a theoretical scenario and do not factor in additional overhead such as activation checkpointing or network latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTX A2000: 93.41 seconds/epoch, 2.55 days required\n",
      "RTX A4000: 34.81 seconds/epoch, 0.95 days required\n",
      "RTX 3090: 15.45 seconds/epoch, 0.42 days required\n",
      "A100 SXM: 4.39 seconds/epoch, 0.12 days required\n",
      "HGX A100: 0.68 seconds/epoch, 0.02 days required\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Device:\n",
    "    name: str\n",
    "    advertised_flops: float\n",
    "    mfu: float\n",
    "\n",
    "    @property\n",
    "    def actual_flops(self) -> float:\n",
    "        return self.mfu * self.advertised_flops\n",
    "\n",
    "\n",
    "devices = [\n",
    "    Device(\"RTX A2000\", 63.9e12, 0.17),\n",
    "    Device(\"RTX A4000\", 153.4e12, 0.19),\n",
    "    Device(\"RTX 3090\", 285.5e12, 0.23),\n",
    "    Device(\"A100 SXM\", 624.0e12, 0.37),\n",
    "    Device(\"HGX A100\", 4992e12, 0.30),\n",
    "]\n",
    "\n",
    "for device in devices:\n",
    "    seconds_per_epoch = samples_per_epoch * total_roundtrip_flops / device.actual_flops\n",
    "\n",
    "    days_required = num_epochs_required * seconds_per_epoch / 60 / 60 / 24\n",
    "\n",
    "    print(\n",
    "        f\"{device.name}: {seconds_per_epoch:.2f} seconds/epoch, {days_required:,.2f} days required\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
