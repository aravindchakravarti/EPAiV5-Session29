{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1DXuhrJz4ssQjL0lEyGBzIO3WVVcLqnjE","authorship_tag":"ABX9TyPR5TFYPvtwSy1kFoLkr5iK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Use KL Divergence loss on Knowledge Distillation Task. You can use any teacher and student model (prefer small models). You need to show that it works, and update README.md with proper logs\n"],"metadata":{"id":"732RXgD-pCXV"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"G8bSmKbGpBfz","executionInfo":{"status":"ok","timestamp":1737630188292,"user_tz":-330,"elapsed":827,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","try:\n","    from torchsummary import summary\n","except ModuleNotFoundError:\n","    !pip install torchsummary\n","    from torchsummary import summary\n","\n","from torchvision import datasets, transforms\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision\n","\n","import os\n","import time\n","import math"]},{"cell_type":"code","source":["train_transforms = transforms.Compose([\n","                                      #  transforms.Resize((28, 28)),\n","                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n","                                      #  transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n","                                       transforms.RandomAffine(degrees=10, shear = 10),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.1307,), (0.3081,))\n","                                       # Note the difference between (0.1307) and (0.1307,)\n","                                       ])\n","\n","# Test Phase transformations\n","test_transforms = transforms.Compose([\n","                                      #  transforms.Resize((28, 28)),\n","                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.1307,), (0.3081,))\n","                                       ])\n","\n","train = datasets.CIFAR10(root = './data', train=True, download=True, transform=train_transforms)\n","test = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n","\n","# Do we have CUDA drivers for us?\n","cuda = torch.cuda.is_available()\n","print (\"Cuda Available?\", cuda)\n","\n","dataloader_args = dict(shuffle=True, batch_size=2048, num_workers=2, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n","\n","# Dataloaders\n","train_loader = torch.utils.data.DataLoader(dataset=train, **dataloader_args)\n","test_loader = torch.utils.data.DataLoader(dataset=test, **dataloader_args)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_yxR2s92RqM","executionInfo":{"status":"ok","timestamp":1737630192887,"user_tz":-330,"elapsed":1489,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"b1c77c4e-38ae-4a6f-ee6c-889646ae1147"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Cuda Available? True\n"]}]},{"cell_type":"code","source":["class TeacherModel(nn.Module):\n","    def __init__(self):\n","        super(TeacherModel, self).__init__()\n","        self.conv01 = nn.Conv2d(3, 16, 3, bias=False, padding=1)\n","        self.batch01 = nn.BatchNorm2d(num_features=16)\n","\n","        # ---- Lets take a skip connection\n","        self.skip_conv1 = nn.Conv2d(16, 16, 3, padding=0, dilation=2)\n","\n","        self.conv02 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch02 = nn.BatchNorm2d(num_features=16)\n","        self.conv03 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch03 = nn.BatchNorm2d(num_features=16)\n","        self.conv04 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch04 = nn.BatchNorm2d(num_features=16)\n","        self.pool01 = nn.MaxPool2d(2, 2)                                #O=16\n","        self.conv05 = nn.Conv2d(16, 16, 1, bias=False)\n","\n","        self.conv11 = nn.Conv2d(16, 64, 3, bias=False, padding=1)\n","        self.batch11 = nn.BatchNorm2d(num_features=64)\n","        self.conv12 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch12 = nn.BatchNorm2d(num_features=64)\n","        self.conv13 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch13 = nn.BatchNorm2d(num_features=64)\n","        self.conv14 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch14 = nn.BatchNorm2d(num_features=64)\n","        self.pool11 = nn.MaxPool2d(2, 2)                                #O=8\n","        self.conv15 = nn.Conv2d(64, 64, 1, bias=False)\n","\n","        self.conv21 = nn.Conv2d(64, 128, 3, bias=False, padding=1)\n","        self.batch21 = nn.BatchNorm2d(num_features=128)\n","        self.conv22 = nn.Conv2d(128, 128, 3, bias=False, padding=1)\n","        self.batch22 = nn.BatchNorm2d(num_features=128)\n","        self.conv23 = nn.Conv2d(128,128, 3, bias=False, padding=1)\n","        self.batch23 = nn.BatchNorm2d(num_features=128)\n","        self.conv24 = nn.Conv2d(128, 128, 3, bias=False, padding=1)\n","        self.batch24 = nn.BatchNorm2d(num_features=128)\n","        self.pool21 = nn.MaxPool2d(2, 2)                                #O=4\n","        self.conv25 = nn.Conv2d(128, 128, 1, bias=False)\n","\n","        self.conv31 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n","        self.convPV1= nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, bias = False, padding = 0)\n","        self.batch31 = nn.BatchNorm2d(num_features=128)\n","        self.conv32 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n","        self.convPV2= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, bias = False, padding = 0)\n","        self.batch32 = nn.BatchNorm2d(num_features=256)\n","\n","\n","        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n","        self.convx3 = nn.Conv2d(256, 10, 1, bias=False, padding=0)\n","\n","    def forward(self, x):\n","        x = self.batch01(F.relu(self.conv01(x)))\n","\n","        # ---- Lets take a skip connection\n","        skip_channels = self.skip_conv1(self.skip_conv1(self.skip_conv1(self.skip_conv1(x))))\n","\n","        x = self.batch02(F.relu(self.conv02(x)))\n","        x = self.batch03(F.relu(self.conv03(x)))\n","        x = self.batch04(F.relu(self.conv04(x)))\n","        x = self.pool01(x)\n","        x = self.conv05(x)\n","        # ----------------------------------------------------------\n","\n","        # ---- Lets add the skip connection here\n","        x = skip_channels + x\n","\n","        x = self.batch11(F.relu(self.conv11(x)))\n","        x = self.batch12(F.relu(self.conv12(x)))\n","        x = self.batch13(F.relu(self.conv13(x)))\n","        x = self.batch14(F.relu(self.conv14(x)))\n","        x = self.pool11(x)\n","        x = self.conv15(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch21(F.relu(self.conv21(x)))\n","        x = self.batch22(F.relu(self.conv22(x)))\n","        x = self.batch23(F.relu(self.conv23(x)))\n","        x = self.batch24(F.relu(self.conv24(x)))\n","        x = self.pool21(x)\n","        x = self.conv25(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch31(F.relu(self.convPV1(F.relu(self.conv31(x)))))\n","        x = self.batch32(F.relu(self.convPV2(F.relu(self.conv32(x)))))\n","\n","\n","        x = self.avg_pool(x)\n","        x = self.convx3(x)\n","        x = x.view(-1, 10)                           # Don't want 10x1x1..\n","        return F.log_softmax(x, dim=1)  # Added dim=1 parameter)"],"metadata":{"id":"K10_ILasJFly","executionInfo":{"status":"ok","timestamp":1737630196339,"user_tz":-330,"elapsed":394,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","time_taken = []\n","\n","class EarlyStopping:\n","    def __init__(self, tolerance=5, min_delta=0.01):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.prev_loss = None  # Initialize as None\n","        self.counter = 0\n","\n","    def __call__(self, train_loss):\n","        if self.prev_loss is None:  # First iteration\n","            self.prev_loss = train_loss\n","            return False  # Continue training\n","\n","        if (abs(train_loss - self.prev_loss)) < self.min_delta:\n","            print(f'---------- prev = {self.prev_loss} current = {train_loss} ---------')\n","            self.counter += 1\n","        else:\n","            self.counter = 0  # Reset counter if loss improves\n","\n","        self.prev_loss = train_loss\n","\n","        return self.counter >= self.tolerance  # Return True if stopping criteria met\n","\n","\n","\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","\n","    correct = 0\n","    processed = 0\n","    epoch_loss = 0\n","    time_taken.clear()\n","\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        t0 = time.time()\n","\n","        data, target = data.to(device), target.to(device)\n","\n","        # Don't want history of gradients\n","        optimizer.zero_grad()\n","\n","        y_predict = model(data)\n","\n","        # Calculate loss\n","        loss = F.nll_loss(y_predict, target)\n","        epoch_loss += loss.item()\n","\n","        # Backpropagate error\n","        loss.backward()\n","\n","        # Take an optimizer step\n","        optimizer.step()\n","\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","\n","        time_taken.append((t1 - t0))\n","\n","        pred = y_predict.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","\n","        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n","        train_acc.append(100 * correct / processed)\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    return avg_train_loss\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","    test_acc.append(100. * correct / len(test_loader.dataset))\n","    return test_loss"],"metadata":{"id":"MMVJZShF7YfE","executionInfo":{"status":"ok","timestamp":1737623869170,"user_tz":-330,"elapsed":707914,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c02bb636-ccdd-4082-bf5c-60cd38b59698"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             432\n","       BatchNorm2d-2           [-1, 16, 32, 32]              32\n","            Conv2d-3           [-1, 16, 28, 28]           2,320\n","            Conv2d-4           [-1, 16, 24, 24]           2,320\n","            Conv2d-5           [-1, 16, 20, 20]           2,320\n","            Conv2d-6           [-1, 16, 16, 16]           2,320\n","            Conv2d-7           [-1, 16, 32, 32]           2,304\n","       BatchNorm2d-8           [-1, 16, 32, 32]              32\n","            Conv2d-9           [-1, 16, 32, 32]           2,304\n","      BatchNorm2d-10           [-1, 16, 32, 32]              32\n","           Conv2d-11           [-1, 16, 32, 32]           2,304\n","      BatchNorm2d-12           [-1, 16, 32, 32]              32\n","        MaxPool2d-13           [-1, 16, 16, 16]               0\n","           Conv2d-14           [-1, 16, 16, 16]             256\n","           Conv2d-15           [-1, 64, 16, 16]           9,216\n","      BatchNorm2d-16           [-1, 64, 16, 16]             128\n","           Conv2d-17           [-1, 64, 16, 16]          36,864\n","      BatchNorm2d-18           [-1, 64, 16, 16]             128\n","           Conv2d-19           [-1, 64, 16, 16]          36,864\n","      BatchNorm2d-20           [-1, 64, 16, 16]             128\n","           Conv2d-21           [-1, 64, 16, 16]          36,864\n","      BatchNorm2d-22           [-1, 64, 16, 16]             128\n","        MaxPool2d-23             [-1, 64, 8, 8]               0\n","           Conv2d-24             [-1, 64, 8, 8]           4,096\n","           Conv2d-25            [-1, 128, 8, 8]          73,728\n","      BatchNorm2d-26            [-1, 128, 8, 8]             256\n","           Conv2d-27            [-1, 128, 8, 8]         147,456\n","      BatchNorm2d-28            [-1, 128, 8, 8]             256\n","           Conv2d-29            [-1, 128, 8, 8]         147,456\n","      BatchNorm2d-30            [-1, 128, 8, 8]             256\n","           Conv2d-31            [-1, 128, 8, 8]         147,456\n","      BatchNorm2d-32            [-1, 128, 8, 8]             256\n","        MaxPool2d-33            [-1, 128, 4, 4]               0\n","           Conv2d-34            [-1, 128, 4, 4]          16,384\n","           Conv2d-35            [-1, 128, 4, 4]           1,152\n","           Conv2d-36            [-1, 128, 4, 4]          16,384\n","      BatchNorm2d-37            [-1, 128, 4, 4]             256\n","           Conv2d-38            [-1, 128, 4, 4]           1,152\n","           Conv2d-39            [-1, 256, 4, 4]          32,768\n","      BatchNorm2d-40            [-1, 256, 4, 4]             512\n","        AvgPool2d-41            [-1, 256, 1, 1]               0\n","           Conv2d-42             [-1, 10, 1, 1]           2,560\n","================================================================\n","Total params: 729,712\n","Trainable params: 729,712\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 3.03\n","Params size (MB): 2.78\n","Estimated Total Size (MB): 5.82\n","----------------------------------------------------------------\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.6608870029449463 Batch_id=24 Accuracy=25.05: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 0, Avg Training Loss: 2.0001, Avg Time Taken = 442.69ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 2.3458, Accuracy: 1373/10000 (13.73%)\n","\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.409379005432129 Batch_id=24 Accuracy=42.47: 100%|██████████| 25/25 [00:20<00:00,  1.24it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 1, Avg Training Loss: 1.5328, Avg Time Taken = 434.34ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.5188, Accuracy: 4367/10000 (43.67%)\n","\n","Epoch 3/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.2821158170700073 Batch_id=24 Accuracy=50.34: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 2, Avg Training Loss: 1.3499, Avg Time Taken = 442.18ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.3797, Accuracy: 4987/10000 (49.87%)\n","\n","Epoch 4/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.1426337957382202 Batch_id=24 Accuracy=56.14: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 3, Avg Training Loss: 1.2080, Avg Time Taken = 456.69ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.1863, Accuracy: 5738/10000 (57.38%)\n","\n","Epoch 5/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.1013754606246948 Batch_id=24 Accuracy=60.83: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 4, Avg Training Loss: 1.0925, Avg Time Taken = 467.66ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.1440, Accuracy: 6050/10000 (60.50%)\n","\n","Epoch 6/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0109670162200928 Batch_id=24 Accuracy=63.88: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 5, Avg Training Loss: 1.0116, Avg Time Taken = 481.71ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.0084, Accuracy: 6420/10000 (64.20%)\n","\n","Epoch 7/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8873741626739502 Batch_id=24 Accuracy=66.47: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 6, Avg Training Loss: 0.9331, Avg Time Taken = 469.92ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.0001, Accuracy: 6456/10000 (64.56%)\n","\n","Epoch 8/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8859184384346008 Batch_id=24 Accuracy=68.99: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 7, Avg Training Loss: 0.8799, Avg Time Taken = 465.46ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9434, Accuracy: 6702/10000 (67.02%)\n","\n","Epoch 9/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8045757412910461 Batch_id=24 Accuracy=71.00: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 8, Avg Training Loss: 0.8151, Avg Time Taken = 470.15ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8705, Accuracy: 7017/10000 (70.17%)\n","\n","Epoch 10/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.7854903340339661 Batch_id=24 Accuracy=73.25: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 9, Avg Training Loss: 0.7588, Avg Time Taken = 471.01ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8487, Accuracy: 7085/10000 (70.85%)\n","\n","Epoch 11/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6698622703552246 Batch_id=24 Accuracy=74.93: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 10, Avg Training Loss: 0.7134, Avg Time Taken = 468.97ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8550, Accuracy: 7061/10000 (70.61%)\n","\n","Epoch 12/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6664386987686157 Batch_id=24 Accuracy=76.31: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 11, Avg Training Loss: 0.6699, Avg Time Taken = 470.06ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8167, Accuracy: 7261/10000 (72.61%)\n","\n","Epoch 13/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5815029740333557 Batch_id=24 Accuracy=77.97: 100%|██████████| 25/25 [00:17<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 12, Avg Training Loss: 0.6254, Avg Time Taken = 472.03ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7687, Accuracy: 7369/10000 (73.69%)\n","\n","Epoch 14/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6217029690742493 Batch_id=24 Accuracy=78.91: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 13, Avg Training Loss: 0.5996, Avg Time Taken = 469.57ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7773, Accuracy: 7391/10000 (73.91%)\n","\n","Epoch 15/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5859531760215759 Batch_id=24 Accuracy=80.35: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 14, Avg Training Loss: 0.5619, Avg Time Taken = 466.26ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7854, Accuracy: 7337/10000 (73.37%)\n","\n","Epoch 16/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5812035799026489 Batch_id=24 Accuracy=81.09: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 15, Avg Training Loss: 0.5406, Avg Time Taken = 475.19ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7305, Accuracy: 7539/10000 (75.39%)\n","\n","Epoch 17/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5477516055107117 Batch_id=24 Accuracy=82.29: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 16, Avg Training Loss: 0.5071, Avg Time Taken = 469.09ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7564, Accuracy: 7483/10000 (74.83%)\n","\n","Epoch 18/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.4621361196041107 Batch_id=24 Accuracy=83.12: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 17, Avg Training Loss: 0.4839, Avg Time Taken = 472.00ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7170, Accuracy: 7680/10000 (76.80%)\n","\n","Epoch 19/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.4255615174770355 Batch_id=24 Accuracy=84.40: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 18, Avg Training Loss: 0.4501, Avg Time Taken = 470.78ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7371, Accuracy: 7624/10000 (76.24%)\n","\n","Epoch 20/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.4585602879524231 Batch_id=24 Accuracy=85.07: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 19, Avg Training Loss: 0.4309, Avg Time Taken = 469.88ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7230, Accuracy: 7672/10000 (76.72%)\n","\n","---------- prev = 0.45011252999305723 current = 0.43086560487747194 ---------\n","Epoch 21/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3736545741558075 Batch_id=24 Accuracy=85.75: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 20, Avg Training Loss: 0.4051, Avg Time Taken = 469.38ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7149, Accuracy: 7655/10000 (76.55%)\n","\n","Epoch 22/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.412638783454895 Batch_id=24 Accuracy=86.63: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 21, Avg Training Loss: 0.3821, Avg Time Taken = 471.27ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7607, Accuracy: 7632/10000 (76.32%)\n","\n","Epoch 23/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.4209372401237488 Batch_id=24 Accuracy=87.34: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 22, Avg Training Loss: 0.3651, Avg Time Taken = 471.82ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7522, Accuracy: 7672/10000 (76.72%)\n","\n","---------- prev = 0.38208768963813783 current = 0.3650553143024445 ---------\n","Epoch 24/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3752506971359253 Batch_id=24 Accuracy=87.81: 100%|██████████| 25/25 [00:17<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 23, Avg Training Loss: 0.3507, Avg Time Taken = 473.14ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6966, Accuracy: 7817/10000 (78.17%)\n","\n","---------- prev = 0.3650553143024445 current = 0.3507401490211487 ---------\n","Epoch 25/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3359489142894745 Batch_id=24 Accuracy=88.23: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 24, Avg Training Loss: 0.3369, Avg Time Taken = 467.33ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7221, Accuracy: 7759/10000 (77.59%)\n","\n","---------- prev = 0.3507401490211487 current = 0.3369123363494873 ---------\n","Epoch 26/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.34969058632850647 Batch_id=24 Accuracy=89.07: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 25, Avg Training Loss: 0.3139, Avg Time Taken = 469.92ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7374, Accuracy: 7726/10000 (77.26%)\n","\n","Epoch 27/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.2797302305698395 Batch_id=24 Accuracy=90.01: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 26, Avg Training Loss: 0.2925, Avg Time Taken = 468.78ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6983, Accuracy: 7830/10000 (78.30%)\n","\n","Epoch 28/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.2557670474052429 Batch_id=24 Accuracy=90.24: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 27, Avg Training Loss: 0.2786, Avg Time Taken = 469.72ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7558, Accuracy: 7702/10000 (77.02%)\n","\n","---------- prev = 0.2925028991699219 current = 0.27855880856513976 ---------\n","Epoch 29/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.2288149744272232 Batch_id=24 Accuracy=91.24: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 28, Avg Training Loss: 0.2550, Avg Time Taken = 469.45ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7281, Accuracy: 7783/10000 (77.83%)\n","\n","Epoch 30/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.22104182839393616 Batch_id=24 Accuracy=91.36: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 29, Avg Training Loss: 0.2484, Avg Time Taken = 468.10ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7941, Accuracy: 7708/10000 (77.08%)\n","\n","---------- prev = 0.25495045244693754 current = 0.24840199291706086 ---------\n","Epoch 31/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.2958418130874634 Batch_id=24 Accuracy=91.63: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 30, Avg Training Loss: 0.2414, Avg Time Taken = 471.31ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7602, Accuracy: 7795/10000 (77.95%)\n","\n","---------- prev = 0.24840199291706086 current = 0.2414081174135208 ---------\n","Epoch 32/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.2600346803665161 Batch_id=24 Accuracy=92.19: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 31, Avg Training Loss: 0.2265, Avg Time Taken = 470.95ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7412, Accuracy: 7839/10000 (78.39%)\n","\n","---------- prev = 0.2414081174135208 current = 0.2265439236164093 ---------\n","Epoch 33/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.213126078248024 Batch_id=24 Accuracy=92.50: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 32, Avg Training Loss: 0.2174, Avg Time Taken = 468.34ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7855, Accuracy: 7799/10000 (77.99%)\n","\n","---------- prev = 0.2265439236164093 current = 0.2173517221212387 ---------\n","Epoch 34/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.20625358819961548 Batch_id=24 Accuracy=92.55: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 33, Avg Training Loss: 0.2151, Avg Time Taken = 468.82ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7590, Accuracy: 7844/10000 (78.44%)\n","\n","---------- prev = 0.2173517221212387 current = 0.2151263326406479 ---------\n","Model saved at: /content/drive/MyDrive/EPAi_V5/model_heavy_acc_92.pth\n","Early stopping triggered!\n"]}]},{"cell_type":"code","source":["# Initialize model, optimizer, and early stopping\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (f'Device Using = {device}')\n","model = TeacherModel().to(device)\n","summary(model, input_size=(3, 32, 32))\n","criteria = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","early_stopping = EarlyStopping(tolerance=5, min_delta=0.02)\n","\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n","    avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n","    print(f\" --> EPOCH: {epoch}, Avg Training Loss: {avg_train_loss:.4f}, Avg Time Taken = {(sum(time_taken) / len(time_taken)) * 1000:.2f}ms\")\n","    val_loss = test(model, device, test_loader)\n","\n","    # Check for early stopping\n","    if early_stopping(avg_train_loss):\n","        try:\n","            # Ensure the directory exists\n","            save_dir = '/content/drive/MyDrive/EPAi_V5'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, f'model_heavy_acc_{int(train_acc[-1]):d}.pth')\n","        except OSError:\n","            # Fallback to current directory if Drive is unavailable\n","            PATH = f'./model_heavy_acc_{int(train_acc[-1]):d}.pth'\n","\n","        # Save the model weights\n","        torch.save(model.state_dict(), PATH)\n","        print(f\"Model saved at: {PATH}\")\n","        print(\"Early stopping triggered!\")\n","        break"],"metadata":{"id":"aoTqX5JC4pSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_heavy_acc_92.pth', weights_only=True))\n","test(model, device, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwjLKJLdAo8d","executionInfo":{"status":"ok","timestamp":1737623933029,"user_tz":-330,"elapsed":3481,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"73bfcc30-a5fd-47cf-9bfc-a34037c98ed6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6850, Accuracy: 7915/10000 (79.15%)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0.6849886352539063"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["class StudentModel(nn.Module):\n","    def __init__(self):\n","        super(StudentModel, self).__init__()\n","        self.conv01 = nn.Conv2d(3, 16, 3, bias=False, padding=1)\n","        self.batch01 = nn.BatchNorm2d(num_features=16)\n","\n","        # ---- Lets take a skip connection\n","        self.skip_conv1 = nn.Conv2d(16, 16, 3, padding=0, dilation=2)\n","\n","        self.conv02 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch02 = nn.BatchNorm2d(num_features=16)\n","        self.conv03 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch03 = nn.BatchNorm2d(num_features=16)\n","        self.conv04 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch04 = nn.BatchNorm2d(num_features=16)\n","        self.pool01 = nn.MaxPool2d(2, 2)                                #O=16\n","        self.conv05 = nn.Conv2d(16, 16, 1, bias=False)\n","\n","        self.conv11 = nn.Conv2d(16, 32, 3, bias=False, padding=1)\n","        self.batch11 = nn.BatchNorm2d(num_features=32)\n","        self.conv12 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n","        self.batch12 = nn.BatchNorm2d(num_features=32)\n","        self.conv13 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n","        self.batch13 = nn.BatchNorm2d(num_features=32)\n","        self.conv14 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n","        self.batch14 = nn.BatchNorm2d(num_features=32)\n","        self.pool11 = nn.MaxPool2d(2, 2)                                #O=8\n","        self.conv15 = nn.Conv2d(32, 32, 1, bias=False)\n","\n","        self.conv21 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n","        self.batch21 = nn.BatchNorm2d(num_features=64)\n","        self.conv22 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch22 = nn.BatchNorm2d(num_features=64)\n","        self.conv23 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch23 = nn.BatchNorm2d(num_features=64)\n","        self.conv24 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch24 = nn.BatchNorm2d(num_features=64)\n","        self.pool21 = nn.MaxPool2d(2, 2)                                #O=4\n","        self.conv25 = nn.Conv2d(64, 64, 1, bias=False)\n","\n","        self.conv31 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, groups=64, bias = False, padding = 1)\n","        self.convPV1= nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, bias = False, padding = 0)\n","        self.batch31 = nn.BatchNorm2d(num_features=128)\n","        self.conv32 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n","        self.convPV2= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, bias = False, padding = 0)\n","        self.batch32 = nn.BatchNorm2d(num_features=256)\n","\n","\n","        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n","        self.convx3 = nn.Conv2d(256, 10, 1, bias=False, padding=0)\n","\n","    def forward(self, x):\n","        x = self.batch01(F.relu(self.conv01(x)))\n","\n","        # ---- Lets take a skip connection\n","        skip_channels = self.skip_conv1(self.skip_conv1(self.skip_conv1(self.skip_conv1(x))))\n","\n","        x = self.batch02(F.relu(self.conv02(x)))\n","        x = self.batch03(F.relu(self.conv03(x)))\n","        x = self.batch04(F.relu(self.conv04(x)))\n","        x = self.pool01(x)\n","        x = self.conv05(x)\n","        # ----------------------------------------------------------\n","\n","        # ---- Lets add the skip connection here\n","        x = skip_channels + x\n","\n","        x = self.batch11(F.relu(self.conv11(x)))\n","        x = self.batch12(F.relu(self.conv12(x)))\n","        x = self.batch13(F.relu(self.conv13(x)))\n","        x = self.batch14(F.relu(self.conv14(x)))\n","        x = self.pool11(x)\n","        x = self.conv15(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch21(F.relu(self.conv21(x)))\n","        x = self.batch22(F.relu(self.conv22(x)))\n","        x = self.batch23(F.relu(self.conv23(x)))\n","        x = self.batch24(F.relu(self.conv24(x)))\n","        x = self.pool21(x)\n","        x = self.conv25(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch31(F.relu(self.convPV1(F.relu(self.conv31(x)))))\n","        x = self.batch32(F.relu(self.convPV2(F.relu(self.conv32(x)))))\n","\n","\n","        x = self.avg_pool(x)\n","        x = self.convx3(x)\n","        x = x.view(-1, 10)                           # Don't want 10x1x1..\n","        return F.log_softmax(x, dim=1)  # Added dim=1 parameter)"],"metadata":{"id":"cXV8NnClBdfj","executionInfo":{"status":"ok","timestamp":1737630225070,"user_tz":-330,"elapsed":403,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","time_taken = []\n","\n","class EarlyStopping:\n","    def __init__(self, tolerance=5, min_delta=0.01):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.prev_loss = None  # Initialize as None\n","        self.counter = 0\n","\n","    def __call__(self, train_loss):\n","        if self.prev_loss is None:  # First iteration\n","            self.prev_loss = train_loss\n","            return False  # Continue training\n","\n","        if (abs(train_loss - self.prev_loss)) < self.min_delta:\n","            print(f'---------- prev = {self.prev_loss} current = {train_loss} ---------')\n","            self.counter += 1\n","        else:\n","            self.counter = 0  # Reset counter if loss improves\n","\n","        self.prev_loss = train_loss\n","\n","        return self.counter >= self.tolerance  # Return True if stopping criteria met\n","\n","\n","\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","\n","    correct = 0\n","    processed = 0\n","    epoch_loss = 0\n","    time_taken.clear()\n","\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        t0 = time.time()\n","\n","        data, target = data.to(device), target.to(device)\n","\n","        # Don't want history of gradients\n","        optimizer.zero_grad()\n","\n","        y_predict = model(data)\n","\n","        # Calculate loss\n","        loss = F.nll_loss(y_predict, target)\n","        epoch_loss += loss.item()\n","\n","        # Backpropagate error\n","        loss.backward()\n","\n","        # Take an optimizer step\n","        optimizer.step()\n","\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","\n","        time_taken.append((t1 - t0))\n","\n","        pred = y_predict.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","\n","        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n","        train_acc.append(100 * correct / processed)\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    return avg_train_loss\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","    test_acc.append(100. * correct / len(test_loader.dataset))\n","    return test_loss"],"metadata":{"id":"CZfihPseBk1x","executionInfo":{"status":"ok","timestamp":1737630372813,"user_tz":-330,"elapsed":383,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Initialize model, optimizer, and early stopping\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (f'Device Using = {device}')\n","model = StudentModel().to(device)\n","summary(model, input_size=(3, 32, 32))\n","criteria = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","early_stopping = EarlyStopping(tolerance=5, min_delta=0.02)\n","\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n","    avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n","    print(f\" --> EPOCH: {epoch}, Avg Training Loss: {avg_train_loss:.4f}, Avg Time Taken = {(sum(time_taken) / len(time_taken)) * 1000:.2f}ms\")\n","    val_loss = test(model, device, test_loader)\n","\n","    # Check for early stopping\n","    if early_stopping(avg_train_loss):\n","        try:\n","            # Ensure the directory exists\n","            save_dir = '/content/drive/MyDrive/EPAi_V5'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, f'model_small_acc_{int(train_acc[-1]):d}.pth')\n","        except OSError:\n","            # Fallback to current directory if Drive is unavailable\n","            PATH = f'./model_small_acc_{int(train_acc[-1]):d}.pth'\n","\n","        # Save the model weights\n","        torch.save(model.state_dict(), PATH)\n","        print(f\"Model saved at: {PATH}\")\n","        print(\"Early stopping triggered!\")\n","        break"],"metadata":{"id":"iwjgpkhj1zhl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_small_acc_81.pth', weights_only=True))\n","test(model, device, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHwivY2wd0WI","executionInfo":{"status":"ok","timestamp":1737624584642,"user_tz":-330,"elapsed":2384,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"8bb2a00c-c5bf-4e6a-b1e2-2cf9fd6c6e7a"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7419, Accuracy: 7566/10000 (75.66%)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7419342895507812"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["# Distillation Process"],"metadata":{"id":"6KbNLvnQgu3V"}},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, tolerance=5, min_delta=0.01):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.prev_loss = None  # Initialize as None\n","        self.counter = 0\n","\n","    def __call__(self, train_loss):\n","        if self.prev_loss is None:  # First iteration\n","            self.prev_loss = train_loss\n","            return False  # Continue training\n","\n","        if (abs(train_loss - self.prev_loss)) < self.min_delta:\n","            print(f'---------- prev = {self.prev_loss} current = {train_loss} ---------')\n","            self.counter += 1\n","        else:\n","            self.counter = 0  # Reset counter if loss improves\n","\n","        self.prev_loss = train_loss\n","\n","        return self.counter >= self.tolerance  # Return True if stopping criteria met\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","time_taken = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device = {device}')\n","\n","# Prepare teacher model\n","teacher = TeacherModel().to(device)\n","teacher.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_heavy_acc_92.pth', weights_only=True))\n","teacher.eval()\n","\n","# Prepare student model\n","student = StudentModel().to(device)\n","student.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_small_acc_81.pth', weights_only=True))\n","\n","print(\"=============================================\")\n","print(\"Student model accuracy before training \")\n","student.eval()\n","test(student, device, test_loader)\n","print(\"=============================================\")\n","\n","# Loss functions\n","hard_loss = nn.CrossEntropyLoss() #Hard label loss\n","soft_loss = nn.KLDivLoss(reduction=\"batchmean\")  # Distillation loss\n","\n","# Temperature and alpha\n","T = 5.0  # Temperature\n","alpha = 0.5  # Weight for distillation loss\n","\n","# Optimizer\n","optimizer = optim.Adam(student.parameters(), lr=0.001)\n","\n","# Training loop\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","\n","    correct = 0\n","    processed = 0\n","    epoch_loss = 0\n","    pbar = tqdm(train_loader)\n","    student.train()\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Teacher predictions (soft labels)\n","        with torch.no_grad():\n","            teacher_logits = teacher(data) / T\n","\n","        teacher_probs = torch.softmax(teacher_logits, dim=1)\n","\n","        # Student predictions\n","        student_logits = student(data)\n","        student_probs = torch.log_softmax(student_logits / T, dim=1)\n","\n","        # Compute losses\n","        loss_soft = soft_loss(student_probs, teacher_probs) * (T ** 2)  # Scale by T^2\n","        loss_hard = hard_loss(student_logits, target)\n","        loss = alpha * loss_hard + (1 - alpha) * loss_soft\n","\n","        epoch_loss += loss.item()\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = student_logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","\n","        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","\n","    # Check for early stopping\n","    if early_stopping(avg_train_loss):\n","        try:\n","            # Ensure the directory exists\n","            save_dir = '/content/drive/MyDrive/EPAi_V5'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, f'model_small_acc_{int(train_acc[-1]):d}.pth')\n","        except OSError:\n","            # Fallback to current directory if Drive is unavailable\n","            PATH = f'./model_small_acc_{int(train_acc[-1]):d}.pth'\n","\n","        # Save the model weights\n","        torch.save(model.state_dict(), PATH)\n","        print(f\"Model saved at: {PATH}\")\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","    student.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = student(data)\n","\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"goXWg5nAgyrO","executionInfo":{"status":"error","timestamp":1737630732755,"user_tz":-330,"elapsed":332762,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"eb33cc52-50d5-47f5-c34b-0cc6647afdad"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device = cuda\n","=============================================\n","Student model accuracy before training \n","\n","Test set: Average loss: 0.7419, Accuracy: 7566/10000 (75.66%)\n","\n","=============================================\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.3155590295791626 Batch_id=24 Accuracy=75.65: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8766, Accuracy: 7533/10000 (75.33%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.1270556449890137 Batch_id=24 Accuracy=80.29: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7717, Accuracy: 7695/10000 (76.95%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.1114906072616577 Batch_id=24 Accuracy=81.71: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7617, Accuracy: 7803/10000 (78.03%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0582565069198608 Batch_id=24 Accuracy=81.95: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7608, Accuracy: 7832/10000 (78.32%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0600461959838867 Batch_id=24 Accuracy=82.41: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7835, Accuracy: 7711/10000 (77.11%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0280518531799316 Batch_id=24 Accuracy=82.47: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7539, Accuracy: 7823/10000 (78.23%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0234251022338867 Batch_id=24 Accuracy=82.91: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7279, Accuracy: 7889/10000 (78.89%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9528346061706543 Batch_id=24 Accuracy=83.53: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7846, Accuracy: 7772/10000 (77.72%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.001296043395996 Batch_id=24 Accuracy=83.85: 100%|██████████| 25/25 [00:17<00:00,  1.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7395, Accuracy: 7865/10000 (78.65%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9289317727088928 Batch_id=24 Accuracy=84.13: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7789, Accuracy: 7763/10000 (77.63%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9482051134109497 Batch_id=24 Accuracy=84.53: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7370, Accuracy: 7930/10000 (79.30%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8285729289054871 Batch_id=24 Accuracy=84.97: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7871, Accuracy: 7793/10000 (77.93%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8407131433486938 Batch_id=24 Accuracy=85.06: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7056, Accuracy: 7932/10000 (79.32%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8000921607017517 Batch_id=24 Accuracy=85.44: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7124, Accuracy: 7936/10000 (79.36%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.818144679069519 Batch_id=24 Accuracy=85.60: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7284, Accuracy: 7913/10000 (79.13%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8630034327507019 Batch_id=24 Accuracy=86.15: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7138, Accuracy: 7928/10000 (79.28%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8151281476020813 Batch_id=5 Accuracy=85.91:  24%|██▍       | 6/25 [00:05<00:16,  1.12it/s]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-c1d8ff3ef8c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1400\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}