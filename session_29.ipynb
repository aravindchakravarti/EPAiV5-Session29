{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1DXuhrJz4ssQjL0lEyGBzIO3WVVcLqnjE","authorship_tag":"ABX9TyNW1mdxLJP/zEqECgycfVq7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Use KL Divergence loss on Knowledge Distillation Task. You can use any teacher and student model (prefer small models). You need to show that it works, and update README.md with proper logs\n"],"metadata":{"id":"732RXgD-pCXV"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"G8bSmKbGpBfz","executionInfo":{"status":"ok","timestamp":1737906064296,"user_tz":-330,"elapsed":9867,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","try:\n","    from torchsummary import summary\n","except ModuleNotFoundError:\n","    !pip install torchsummary\n","    from torchsummary import summary\n","\n","from torchvision import datasets, transforms\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torchvision\n","\n","import os\n","import time\n","import math"]},{"cell_type":"code","source":["train_transforms = transforms.Compose([\n","                                      #  transforms.Resize((28, 28)),\n","                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n","                                      #  transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n","                                       transforms.RandomAffine(degrees=10, shear = 10),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.1307,), (0.3081,))\n","                                       # Note the difference between (0.1307) and (0.1307,)\n","                                       ])\n","\n","# Test Phase transformations\n","test_transforms = transforms.Compose([\n","                                      #  transforms.Resize((28, 28)),\n","                                      #  transforms.ColorJitter(brightness=0.10, contrast=0.1, saturation=0.10, hue=0.1),\n","                                       transforms.ToTensor(),\n","                                       transforms.Normalize((0.1307,), (0.3081,))\n","                                       ])\n","\n","train = datasets.CIFAR10(root = './data', train=True, download=True, transform=train_transforms)\n","test = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n","\n","# Do we have CUDA drivers for us?\n","cuda = torch.cuda.is_available()\n","print (\"Cuda Available?\", cuda)\n","\n","dataloader_args = dict(shuffle=True, batch_size=2048, num_workers=2, pin_memory=True) if cuda else dict(shuffle=True, batch_size=64)\n","\n","# Dataloaders\n","train_loader = torch.utils.data.DataLoader(dataset=train, **dataloader_args)\n","test_loader = torch.utils.data.DataLoader(dataset=test, **dataloader_args)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_yxR2s92RqM","executionInfo":{"status":"ok","timestamp":1737906077958,"user_tz":-330,"elapsed":10571,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"40d9cbf2-f824-49d2-93d2-30a943f8a330"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170M/170M [00:05<00:00, 29.7MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","Cuda Available? True\n"]}]},{"cell_type":"code","source":["class TeacherModel(nn.Module):\n","    def __init__(self):\n","        super(TeacherModel, self).__init__()\n","        self.conv01 = nn.Conv2d(3, 16, 3, bias=False, padding=1)\n","        self.batch01 = nn.BatchNorm2d(num_features=16)\n","\n","        # ---- Lets take a skip connection\n","        self.skip_conv1 = nn.Conv2d(16, 16, 3, padding=0, dilation=2)\n","\n","        self.conv02 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch02 = nn.BatchNorm2d(num_features=16)\n","        self.conv03 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch03 = nn.BatchNorm2d(num_features=16)\n","        self.conv04 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch04 = nn.BatchNorm2d(num_features=16)\n","        self.pool01 = nn.MaxPool2d(2, 2)                                #O=16\n","        self.conv05 = nn.Conv2d(16, 16, 1, bias=False)\n","\n","        self.conv11 = nn.Conv2d(16, 64, 3, bias=False, padding=1)\n","        self.batch11 = nn.BatchNorm2d(num_features=64)\n","        self.conv12 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch12 = nn.BatchNorm2d(num_features=64)\n","        self.conv13 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch13 = nn.BatchNorm2d(num_features=64)\n","        self.conv14 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch14 = nn.BatchNorm2d(num_features=64)\n","        self.pool11 = nn.MaxPool2d(2, 2)                                #O=8\n","        self.conv15 = nn.Conv2d(64, 64, 1, bias=False)\n","\n","        self.conv21 = nn.Conv2d(64, 128, 3, bias=False, padding=1)\n","        self.batch21 = nn.BatchNorm2d(num_features=128)\n","        self.conv22 = nn.Conv2d(128, 128, 3, bias=False, padding=1)\n","        self.batch22 = nn.BatchNorm2d(num_features=128)\n","        self.conv23 = nn.Conv2d(128,128, 3, bias=False, padding=1)\n","        self.batch23 = nn.BatchNorm2d(num_features=128)\n","        self.conv24 = nn.Conv2d(128, 128, 3, bias=False, padding=1)\n","        self.batch24 = nn.BatchNorm2d(num_features=128)\n","        self.pool21 = nn.MaxPool2d(2, 2)                                #O=4\n","        self.conv25 = nn.Conv2d(128, 128, 1, bias=False)\n","\n","        self.conv31 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n","        self.convPV1= nn.Conv2d(in_channels=128, out_channels=128, kernel_size=1, bias = False, padding = 0)\n","        self.batch31 = nn.BatchNorm2d(num_features=128)\n","        self.conv32 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n","        self.convPV2= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, bias = False, padding = 0)\n","        self.batch32 = nn.BatchNorm2d(num_features=256)\n","\n","\n","        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n","        self.convx3 = nn.Conv2d(256, 10, 1, bias=False, padding=0)\n","\n","    def forward(self, x):\n","        x = self.batch01(F.relu(self.conv01(x)))\n","\n","        # ---- Lets take a skip connection\n","        skip_channels = self.skip_conv1(self.skip_conv1(self.skip_conv1(self.skip_conv1(x))))\n","\n","        x = self.batch02(F.relu(self.conv02(x)))\n","        x = self.batch03(F.relu(self.conv03(x)))\n","        x = self.batch04(F.relu(self.conv04(x)))\n","        x = self.pool01(x)\n","        x = self.conv05(x)\n","        # ----------------------------------------------------------\n","\n","        # ---- Lets add the skip connection here\n","        x = skip_channels + x\n","\n","        x = self.batch11(F.relu(self.conv11(x)))\n","        x = self.batch12(F.relu(self.conv12(x)))\n","        x = self.batch13(F.relu(self.conv13(x)))\n","        x = self.batch14(F.relu(self.conv14(x)))\n","        x = self.pool11(x)\n","        x = self.conv15(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch21(F.relu(self.conv21(x)))\n","        x = self.batch22(F.relu(self.conv22(x)))\n","        x = self.batch23(F.relu(self.conv23(x)))\n","        x = self.batch24(F.relu(self.conv24(x)))\n","        x = self.pool21(x)\n","        x = self.conv25(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch31(F.relu(self.convPV1(F.relu(self.conv31(x)))))\n","        x = self.batch32(F.relu(self.convPV2(F.relu(self.conv32(x)))))\n","\n","\n","        x = self.avg_pool(x)\n","        x = self.convx3(x)\n","        x = x.view(-1, 10)                           # Don't want 10x1x1..\n","        return F.log_softmax(x, dim=1)  # Added dim=1 parameter)"],"metadata":{"id":"K10_ILasJFly","executionInfo":{"status":"ok","timestamp":1737906118126,"user_tz":-330,"elapsed":369,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","time_taken = []\n","\n","class EarlyStopping:\n","    def __init__(self, tolerance=5, min_delta=0.01):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.prev_loss = None  # Initialize as None\n","        self.counter = 0\n","\n","    def __call__(self, train_loss):\n","        if self.prev_loss is None:  # First iteration\n","            self.prev_loss = train_loss\n","            return False  # Continue training\n","\n","        if (abs(train_loss - self.prev_loss)) < self.min_delta:\n","            print(f'---------- prev = {self.prev_loss} current = {train_loss} ---------')\n","            self.counter += 1\n","        else:\n","            self.counter = 0  # Reset counter if loss improves\n","\n","        self.prev_loss = train_loss\n","\n","        return self.counter >= self.tolerance  # Return True if stopping criteria met\n","\n","\n","\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","\n","    correct = 0\n","    processed = 0\n","    epoch_loss = 0\n","    time_taken.clear()\n","\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        t0 = time.time()\n","\n","        data, target = data.to(device), target.to(device)\n","\n","        # Don't want history of gradients\n","        optimizer.zero_grad()\n","\n","        y_predict = model(data)\n","\n","        # Calculate loss\n","        loss = F.nll_loss(y_predict, target)\n","        epoch_loss += loss.item()\n","\n","        # Backpropagate error\n","        loss.backward()\n","\n","        # Take an optimizer step\n","        optimizer.step()\n","\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","\n","        time_taken.append((t1 - t0))\n","\n","        pred = y_predict.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","\n","        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n","        train_acc.append(100 * correct / processed)\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    return avg_train_loss\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","    test_acc.append(100. * correct / len(test_loader.dataset))\n","    return test_loss"],"metadata":{"id":"MMVJZShF7YfE","executionInfo":{"status":"ok","timestamp":1737906120422,"user_tz":-330,"elapsed":396,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Initialize model, optimizer, and early stopping\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (f'Device Using = {device}')\n","model = TeacherModel().to(device)\n","summary(model, input_size=(3, 32, 32))\n","criteria = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","early_stopping = EarlyStopping(tolerance=5, min_delta=0.02)\n","\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n","    avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n","    print(f\" --> EPOCH: {epoch}, Avg Training Loss: {avg_train_loss:.4f}, Avg Time Taken = {(sum(time_taken) / len(time_taken)) * 1000:.2f}ms\")\n","    val_loss = test(model, device, test_loader)\n","\n","    # Check for early stopping\n","    if early_stopping(avg_train_loss):\n","        try:\n","            # Ensure the directory exists\n","            save_dir = '/content/drive/MyDrive/EPAi_V5'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, f'model_heavy_acc_{int(train_acc[-1]):d}.pth')\n","        except OSError:\n","            # Fallback to current directory if Drive is unavailable\n","            PATH = f'./model_heavy_acc_{int(train_acc[-1]):d}.pth'\n","\n","        # Save the model weights\n","        torch.save(model.state_dict(), PATH)\n","        print(f\"Model saved at: {PATH}\")\n","        print(\"Early stopping triggered!\")\n","        break"],"metadata":{"id":"aoTqX5JC4pSx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737906752087,"user_tz":-330,"elapsed":626341,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"a7ce38ea-02b7-437a-bf2d-9b585159c4e8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Device Using = cuda\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             432\n","       BatchNorm2d-2           [-1, 16, 32, 32]              32\n","            Conv2d-3           [-1, 16, 28, 28]           2,320\n","            Conv2d-4           [-1, 16, 24, 24]           2,320\n","            Conv2d-5           [-1, 16, 20, 20]           2,320\n","            Conv2d-6           [-1, 16, 16, 16]           2,320\n","            Conv2d-7           [-1, 16, 32, 32]           2,304\n","       BatchNorm2d-8           [-1, 16, 32, 32]              32\n","            Conv2d-9           [-1, 16, 32, 32]           2,304\n","      BatchNorm2d-10           [-1, 16, 32, 32]              32\n","           Conv2d-11           [-1, 16, 32, 32]           2,304\n","      BatchNorm2d-12           [-1, 16, 32, 32]              32\n","        MaxPool2d-13           [-1, 16, 16, 16]               0\n","           Conv2d-14           [-1, 16, 16, 16]             256\n","           Conv2d-15           [-1, 64, 16, 16]           9,216\n","      BatchNorm2d-16           [-1, 64, 16, 16]             128\n","           Conv2d-17           [-1, 64, 16, 16]          36,864\n","      BatchNorm2d-18           [-1, 64, 16, 16]             128\n","           Conv2d-19           [-1, 64, 16, 16]          36,864\n","      BatchNorm2d-20           [-1, 64, 16, 16]             128\n","           Conv2d-21           [-1, 64, 16, 16]          36,864\n","      BatchNorm2d-22           [-1, 64, 16, 16]             128\n","        MaxPool2d-23             [-1, 64, 8, 8]               0\n","           Conv2d-24             [-1, 64, 8, 8]           4,096\n","           Conv2d-25            [-1, 128, 8, 8]          73,728\n","      BatchNorm2d-26            [-1, 128, 8, 8]             256\n","           Conv2d-27            [-1, 128, 8, 8]         147,456\n","      BatchNorm2d-28            [-1, 128, 8, 8]             256\n","           Conv2d-29            [-1, 128, 8, 8]         147,456\n","      BatchNorm2d-30            [-1, 128, 8, 8]             256\n","           Conv2d-31            [-1, 128, 8, 8]         147,456\n","      BatchNorm2d-32            [-1, 128, 8, 8]             256\n","        MaxPool2d-33            [-1, 128, 4, 4]               0\n","           Conv2d-34            [-1, 128, 4, 4]          16,384\n","           Conv2d-35            [-1, 128, 4, 4]           1,152\n","           Conv2d-36            [-1, 128, 4, 4]          16,384\n","      BatchNorm2d-37            [-1, 128, 4, 4]             256\n","           Conv2d-38            [-1, 128, 4, 4]           1,152\n","           Conv2d-39            [-1, 256, 4, 4]          32,768\n","      BatchNorm2d-40            [-1, 256, 4, 4]             512\n","        AvgPool2d-41            [-1, 256, 1, 1]               0\n","           Conv2d-42             [-1, 10, 1, 1]           2,560\n","================================================================\n","Total params: 729,712\n","Trainable params: 729,712\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 3.03\n","Params size (MB): 2.78\n","Estimated Total Size (MB): 5.82\n","----------------------------------------------------------------\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.7569657564163208 Batch_id=24 Accuracy=24.23: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 0, Avg Training Loss: 2.0317, Avg Time Taken = 428.80ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 2.3616, Accuracy: 1000/10000 (10.00%)\n","\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.489882469177246 Batch_id=24 Accuracy=42.15: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 1, Avg Training Loss: 1.5687, Avg Time Taken = 417.93ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.6134, Accuracy: 4367/10000 (43.67%)\n","\n","Epoch 3/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.2746210098266602 Batch_id=24 Accuracy=50.72: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 2, Avg Training Loss: 1.3426, Avg Time Taken = 421.68ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.2827, Accuracy: 5303/10000 (53.03%)\n","\n","Epoch 4/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.1759039163589478 Batch_id=24 Accuracy=56.56: 100%|██████████| 25/25 [00:18<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 3, Avg Training Loss: 1.2008, Avg Time Taken = 427.61ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.1646, Accuracy: 5778/10000 (57.78%)\n","\n","Epoch 5/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0140554904937744 Batch_id=24 Accuracy=61.13: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 4, Avg Training Loss: 1.0761, Avg Time Taken = 430.93ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.1328, Accuracy: 6031/10000 (60.31%)\n","\n","Epoch 6/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9696936011314392 Batch_id=24 Accuracy=64.40: 100%|██████████| 25/25 [00:19<00:00,  1.31it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 5, Avg Training Loss: 0.9969, Avg Time Taken = 439.18ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.0748, Accuracy: 6167/10000 (61.67%)\n","\n","Epoch 7/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9940269589424133 Batch_id=24 Accuracy=67.41: 100%|██████████| 25/25 [00:17<00:00,  1.41it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 6, Avg Training Loss: 0.9157, Avg Time Taken = 436.29ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9721, Accuracy: 6614/10000 (66.14%)\n","\n","Epoch 8/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8761070370674133 Batch_id=24 Accuracy=70.04: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 7, Avg Training Loss: 0.8490, Avg Time Taken = 447.69ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9209, Accuracy: 6768/10000 (67.68%)\n","\n","Epoch 9/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.7703757286071777 Batch_id=24 Accuracy=72.27: 100%|██████████| 25/25 [00:17<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 8, Avg Training Loss: 0.7875, Avg Time Taken = 446.57ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8902, Accuracy: 6950/10000 (69.50%)\n","\n","Epoch 10/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.7251912355422974 Batch_id=24 Accuracy=74.38: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 9, Avg Training Loss: 0.7319, Avg Time Taken = 454.60ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8283, Accuracy: 7105/10000 (71.05%)\n","\n","Epoch 11/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6894527673721313 Batch_id=24 Accuracy=75.46: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 10, Avg Training Loss: 0.6942, Avg Time Taken = 457.98ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8576, Accuracy: 7105/10000 (71.05%)\n","\n","Epoch 12/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6242570877075195 Batch_id=24 Accuracy=77.32: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 11, Avg Training Loss: 0.6459, Avg Time Taken = 457.65ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8157, Accuracy: 7270/10000 (72.70%)\n","\n","Epoch 13/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6227222084999084 Batch_id=24 Accuracy=78.55: 100%|██████████| 25/25 [00:19<00:00,  1.28it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 12, Avg Training Loss: 0.6134, Avg Time Taken = 465.67ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7652, Accuracy: 7420/10000 (74.20%)\n","\n","Epoch 14/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5683180093765259 Batch_id=24 Accuracy=79.65: 100%|██████████| 25/25 [00:18<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 13, Avg Training Loss: 0.5806, Avg Time Taken = 469.83ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7777, Accuracy: 7338/10000 (73.38%)\n","\n","Epoch 15/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5646330714225769 Batch_id=24 Accuracy=80.92: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 14, Avg Training Loss: 0.5502, Avg Time Taken = 466.73ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7263, Accuracy: 7560/10000 (75.60%)\n","\n","Epoch 16/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.532381534576416 Batch_id=24 Accuracy=82.18: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 15, Avg Training Loss: 0.5161, Avg Time Taken = 463.41ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7111, Accuracy: 7583/10000 (75.83%)\n","\n","Epoch 17/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5037235021591187 Batch_id=24 Accuracy=82.92: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 16, Avg Training Loss: 0.4924, Avg Time Taken = 467.92ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6844, Accuracy: 7706/10000 (77.06%)\n","\n","Epoch 18/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5057856440544128 Batch_id=24 Accuracy=83.41: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 17, Avg Training Loss: 0.4694, Avg Time Taken = 467.64ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7211, Accuracy: 7617/10000 (76.17%)\n","\n","Epoch 19/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.47667601704597473 Batch_id=24 Accuracy=84.72: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 18, Avg Training Loss: 0.4428, Avg Time Taken = 465.76ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6841, Accuracy: 7704/10000 (77.04%)\n","\n","Epoch 20/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.43096408247947693 Batch_id=24 Accuracy=85.32: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 19, Avg Training Loss: 0.4209, Avg Time Taken = 465.79ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6644, Accuracy: 7787/10000 (77.87%)\n","\n","Epoch 21/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.41643834114074707 Batch_id=24 Accuracy=85.98: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 20, Avg Training Loss: 0.4022, Avg Time Taken = 467.56ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7021, Accuracy: 7748/10000 (77.48%)\n","\n","---------- prev = 0.4209310495853424 current = 0.4022049951553345 ---------\n","Epoch 22/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.43559035658836365 Batch_id=24 Accuracy=87.19: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 21, Avg Training Loss: 0.3750, Avg Time Taken = 466.96ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7172, Accuracy: 7676/10000 (76.76%)\n","\n","Epoch 23/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.380073606967926 Batch_id=24 Accuracy=87.66: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 22, Avg Training Loss: 0.3560, Avg Time Taken = 466.01ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7064, Accuracy: 7730/10000 (77.30%)\n","\n","---------- prev = 0.3750325775146484 current = 0.3559500801563263 ---------\n","Epoch 24/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3013826310634613 Batch_id=24 Accuracy=88.52: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 23, Avg Training Loss: 0.3357, Avg Time Taken = 468.97ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7457, Accuracy: 7755/10000 (77.55%)\n","\n","Epoch 25/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3540545701980591 Batch_id=24 Accuracy=89.29: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 24, Avg Training Loss: 0.3128, Avg Time Taken = 466.98ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7052, Accuracy: 7764/10000 (77.64%)\n","\n","Epoch 26/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.34189483523368835 Batch_id=24 Accuracy=89.44: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 25, Avg Training Loss: 0.3084, Avg Time Taken = 468.49ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7343, Accuracy: 7752/10000 (77.52%)\n","\n","---------- prev = 0.3127818727493286 current = 0.3083853161334991 ---------\n","Epoch 27/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3049859404563904 Batch_id=24 Accuracy=90.14: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 26, Avg Training Loss: 0.2893, Avg Time Taken = 466.61ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6879, Accuracy: 7872/10000 (78.72%)\n","\n","---------- prev = 0.3083853161334991 current = 0.28933569669723513 ---------\n","Epoch 28/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.26368948817253113 Batch_id=24 Accuracy=90.70: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 27, Avg Training Loss: 0.2702, Avg Time Taken = 464.59ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7678, Accuracy: 7724/10000 (77.24%)\n","\n","---------- prev = 0.28933569669723513 current = 0.27017143309116365 ---------\n","Epoch 29/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3211618959903717 Batch_id=24 Accuracy=91.05: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 28, Avg Training Loss: 0.2634, Avg Time Taken = 467.57ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7115, Accuracy: 7846/10000 (78.46%)\n","\n","---------- prev = 0.27017143309116365 current = 0.26343166947364804 ---------\n","Epoch 30/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.3437854051589966 Batch_id=24 Accuracy=91.23: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 29, Avg Training Loss: 0.2569, Avg Time Taken = 465.19ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7036, Accuracy: 7891/10000 (78.91%)\n","\n","---------- prev = 0.26343166947364804 current = 0.2568626445531845 ---------\n","Model saved at: /content/drive/MyDrive/EPAi_V5/model_heavy_acc_91.pth\n","Early stopping triggered!\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_heavy_acc_91.pth', weights_only=True))\n","test(model, device, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwjLKJLdAo8d","executionInfo":{"status":"ok","timestamp":1737906800510,"user_tz":-330,"elapsed":2499,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"0b707b12-bf1f-454d-b169-86cda456bc3b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7036, Accuracy: 7891/10000 (78.91%)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7035769897460937"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["class StudentModel(nn.Module):\n","    def __init__(self):\n","        super(StudentModel, self).__init__()\n","        self.conv01 = nn.Conv2d(3, 16, 3, bias=False, padding=1)\n","        self.batch01 = nn.BatchNorm2d(num_features=16)\n","\n","        # ---- Lets take a skip connection\n","        self.skip_conv1 = nn.Conv2d(16, 16, 3, padding=0, dilation=2)\n","\n","        self.conv02 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch02 = nn.BatchNorm2d(num_features=16)\n","        self.conv03 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch03 = nn.BatchNorm2d(num_features=16)\n","        self.conv04 = nn.Conv2d(16, 16, 3, bias=False,padding=1)\n","        self.batch04 = nn.BatchNorm2d(num_features=16)\n","        self.pool01 = nn.MaxPool2d(2, 2)                                #O=16\n","        self.conv05 = nn.Conv2d(16, 16, 1, bias=False)\n","\n","        self.conv11 = nn.Conv2d(16, 32, 3, bias=False, padding=1)\n","        self.batch11 = nn.BatchNorm2d(num_features=32)\n","        self.conv12 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n","        self.batch12 = nn.BatchNorm2d(num_features=32)\n","        self.conv13 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n","        self.batch13 = nn.BatchNorm2d(num_features=32)\n","        self.conv14 = nn.Conv2d(32, 32, 3, bias=False, padding=1)\n","        self.batch14 = nn.BatchNorm2d(num_features=32)\n","        self.pool11 = nn.MaxPool2d(2, 2)                                #O=8\n","        self.conv15 = nn.Conv2d(32, 32, 1, bias=False)\n","\n","        self.conv21 = nn.Conv2d(32, 64, 3, bias=False, padding=1)\n","        self.batch21 = nn.BatchNorm2d(num_features=64)\n","        self.conv22 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch22 = nn.BatchNorm2d(num_features=64)\n","        self.conv23 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch23 = nn.BatchNorm2d(num_features=64)\n","        self.conv24 = nn.Conv2d(64, 64, 3, bias=False, padding=1)\n","        self.batch24 = nn.BatchNorm2d(num_features=64)\n","        self.pool21 = nn.MaxPool2d(2, 2)                                #O=4\n","        self.conv25 = nn.Conv2d(64, 64, 1, bias=False)\n","\n","        self.conv31 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, groups=64, bias = False, padding = 1)\n","        self.convPV1= nn.Conv2d(in_channels=64, out_channels=128, kernel_size=1, bias = False, padding = 0)\n","        self.batch31 = nn.BatchNorm2d(num_features=128)\n","        self.conv32 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, groups=128, bias = False, padding = 1)\n","        self.convPV2= nn.Conv2d(in_channels=128, out_channels=256, kernel_size=1, bias = False, padding = 0)\n","        self.batch32 = nn.BatchNorm2d(num_features=256)\n","\n","\n","        self.avg_pool = nn.AvgPool2d(kernel_size=4)\n","        self.convx3 = nn.Conv2d(256, 10, 1, bias=False, padding=0)\n","\n","    def forward(self, x):\n","        x = self.batch01(F.relu(self.conv01(x)))\n","\n","        # ---- Lets take a skip connection\n","        skip_channels = self.skip_conv1(self.skip_conv1(self.skip_conv1(self.skip_conv1(x))))\n","\n","        x = self.batch02(F.relu(self.conv02(x)))\n","        x = self.batch03(F.relu(self.conv03(x)))\n","        x = self.batch04(F.relu(self.conv04(x)))\n","        x = self.pool01(x)\n","        x = self.conv05(x)\n","        # ----------------------------------------------------------\n","\n","        # ---- Lets add the skip connection here\n","        x = skip_channels + x\n","\n","        x = self.batch11(F.relu(self.conv11(x)))\n","        x = self.batch12(F.relu(self.conv12(x)))\n","        x = self.batch13(F.relu(self.conv13(x)))\n","        x = self.batch14(F.relu(self.conv14(x)))\n","        x = self.pool11(x)\n","        x = self.conv15(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch21(F.relu(self.conv21(x)))\n","        x = self.batch22(F.relu(self.conv22(x)))\n","        x = self.batch23(F.relu(self.conv23(x)))\n","        x = self.batch24(F.relu(self.conv24(x)))\n","        x = self.pool21(x)\n","        x = self.conv25(x)\n","        # ----------------------------------------------------------\n","\n","        x = self.batch31(F.relu(self.convPV1(F.relu(self.conv31(x)))))\n","        x = self.batch32(F.relu(self.convPV2(F.relu(self.conv32(x)))))\n","\n","\n","        x = self.avg_pool(x)\n","        x = self.convx3(x)\n","        x = x.view(-1, 10)                           # Don't want 10x1x1..\n","        return F.log_softmax(x, dim=1)  # Added dim=1 parameter)"],"metadata":{"id":"cXV8NnClBdfj","executionInfo":{"status":"ok","timestamp":1737906813118,"user_tz":-330,"elapsed":404,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","time_taken = []\n","\n","class EarlyStopping:\n","    def __init__(self, tolerance=5, min_delta=0.01):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.prev_loss = None  # Initialize as None\n","        self.counter = 0\n","\n","    def __call__(self, train_loss):\n","        if self.prev_loss is None:  # First iteration\n","            self.prev_loss = train_loss\n","            return False  # Continue training\n","\n","        if (abs(train_loss - self.prev_loss)) < self.min_delta:\n","            print(f'---------- prev = {self.prev_loss} current = {train_loss} ---------')\n","            self.counter += 1\n","        else:\n","            self.counter = 0  # Reset counter if loss improves\n","\n","        self.prev_loss = train_loss\n","\n","        return self.counter >= self.tolerance  # Return True if stopping criteria met\n","\n","\n","\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    pbar = tqdm(train_loader)\n","\n","    correct = 0\n","    processed = 0\n","    epoch_loss = 0\n","    time_taken.clear()\n","\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        t0 = time.time()\n","\n","        data, target = data.to(device), target.to(device)\n","\n","        # Don't want history of gradients\n","        optimizer.zero_grad()\n","\n","        y_predict = model(data)\n","\n","        # Calculate loss\n","        loss = F.nll_loss(y_predict, target)\n","        epoch_loss += loss.item()\n","\n","        # Backpropagate error\n","        loss.backward()\n","\n","        # Take an optimizer step\n","        optimizer.step()\n","\n","        torch.cuda.synchronize()\n","        t1 = time.time()\n","\n","        time_taken.append((t1 - t0))\n","\n","        pred = y_predict.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","\n","        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n","        train_acc.append(100 * correct / processed)\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","    return avg_train_loss\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = model(data)\n","\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n","    test_acc.append(100. * correct / len(test_loader.dataset))\n","    return test_loss"],"metadata":{"id":"CZfihPseBk1x","executionInfo":{"status":"ok","timestamp":1737906818068,"user_tz":-330,"elapsed":619,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Initialize model, optimizer, and early stopping\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print (f'Device Using = {device}')\n","model = StudentModel().to(device)\n","summary(model, input_size=(3, 32, 32))\n","criteria = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","early_stopping = EarlyStopping(tolerance=5, min_delta=0.02)\n","\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n","    avg_train_loss = train(model, device, train_loader, optimizer, epoch)\n","    print(f\" --> EPOCH: {epoch}, Avg Training Loss: {avg_train_loss:.4f}, Avg Time Taken = {(sum(time_taken) / len(time_taken)) * 1000:.2f}ms\")\n","    val_loss = test(model, device, test_loader)\n","\n","    # Check for early stopping\n","    if early_stopping(avg_train_loss):\n","        try:\n","            # Ensure the directory exists\n","            save_dir = '/content/drive/MyDrive/EPAi_V5'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, f'model_small_acc_{int(train_acc[-1]):d}.pth')\n","        except OSError:\n","            # Fallback to current directory if Drive is unavailable\n","            PATH = f'./model_small_acc_{int(train_acc[-1]):d}.pth'\n","\n","        # Save the model weights\n","        torch.save(model.state_dict(), PATH)\n","        print(f\"Model saved at: {PATH}\")\n","        print(\"Early stopping triggered!\")\n","        break"],"metadata":{"id":"iwjgpkhj1zhl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737907362386,"user_tz":-330,"elapsed":539812,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"43cc56e2-5960-4ebf-8f29-71508f8353dd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Device Using = cuda\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 32, 32]             432\n","       BatchNorm2d-2           [-1, 16, 32, 32]              32\n","            Conv2d-3           [-1, 16, 28, 28]           2,320\n","            Conv2d-4           [-1, 16, 24, 24]           2,320\n","            Conv2d-5           [-1, 16, 20, 20]           2,320\n","            Conv2d-6           [-1, 16, 16, 16]           2,320\n","            Conv2d-7           [-1, 16, 32, 32]           2,304\n","       BatchNorm2d-8           [-1, 16, 32, 32]              32\n","            Conv2d-9           [-1, 16, 32, 32]           2,304\n","      BatchNorm2d-10           [-1, 16, 32, 32]              32\n","           Conv2d-11           [-1, 16, 32, 32]           2,304\n","      BatchNorm2d-12           [-1, 16, 32, 32]              32\n","        MaxPool2d-13           [-1, 16, 16, 16]               0\n","           Conv2d-14           [-1, 16, 16, 16]             256\n","           Conv2d-15           [-1, 32, 16, 16]           4,608\n","      BatchNorm2d-16           [-1, 32, 16, 16]              64\n","           Conv2d-17           [-1, 32, 16, 16]           9,216\n","      BatchNorm2d-18           [-1, 32, 16, 16]              64\n","           Conv2d-19           [-1, 32, 16, 16]           9,216\n","      BatchNorm2d-20           [-1, 32, 16, 16]              64\n","           Conv2d-21           [-1, 32, 16, 16]           9,216\n","      BatchNorm2d-22           [-1, 32, 16, 16]              64\n","        MaxPool2d-23             [-1, 32, 8, 8]               0\n","           Conv2d-24             [-1, 32, 8, 8]           1,024\n","           Conv2d-25             [-1, 64, 8, 8]          18,432\n","      BatchNorm2d-26             [-1, 64, 8, 8]             128\n","           Conv2d-27             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-28             [-1, 64, 8, 8]             128\n","           Conv2d-29             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-30             [-1, 64, 8, 8]             128\n","           Conv2d-31             [-1, 64, 8, 8]          36,864\n","      BatchNorm2d-32             [-1, 64, 8, 8]             128\n","        MaxPool2d-33             [-1, 64, 4, 4]               0\n","           Conv2d-34             [-1, 64, 4, 4]           4,096\n","           Conv2d-35             [-1, 64, 4, 4]             576\n","           Conv2d-36            [-1, 128, 4, 4]           8,192\n","      BatchNorm2d-37            [-1, 128, 4, 4]             256\n","           Conv2d-38            [-1, 128, 4, 4]           1,152\n","           Conv2d-39            [-1, 256, 4, 4]          32,768\n","      BatchNorm2d-40            [-1, 256, 4, 4]             512\n","        AvgPool2d-41            [-1, 256, 1, 1]               0\n","           Conv2d-42             [-1, 10, 1, 1]           2,560\n","================================================================\n","Total params: 230,192\n","Trainable params: 230,192\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 2.22\n","Params size (MB): 0.88\n","Estimated Total Size (MB): 3.11\n","----------------------------------------------------------------\n","Epoch 1/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.7643420696258545 Batch_id=24 Accuracy=22.57: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 0, Avg Training Loss: 2.0624, Avg Time Taken = 357.53ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 2.3772, Accuracy: 1000/10000 (10.00%)\n","\n","Epoch 2/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.5391430854797363 Batch_id=24 Accuracy=37.13: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 1, Avg Training Loss: 1.6691, Avg Time Taken = 359.02ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.6755, Accuracy: 3922/10000 (39.22%)\n","\n","Epoch 3/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.4422804117202759 Batch_id=24 Accuracy=45.17: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 2, Avg Training Loss: 1.4835, Avg Time Taken = 355.18ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.4565, Accuracy: 4640/10000 (46.40%)\n","\n","Epoch 4/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.2985095977783203 Batch_id=24 Accuracy=50.64: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 3, Avg Training Loss: 1.3461, Avg Time Taken = 361.12ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.2908, Accuracy: 5257/10000 (52.57%)\n","\n","Epoch 5/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.238024115562439 Batch_id=24 Accuracy=54.64: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 4, Avg Training Loss: 1.2474, Avg Time Taken = 362.41ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.2401, Accuracy: 5520/10000 (55.20%)\n","\n","Epoch 6/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0856209993362427 Batch_id=24 Accuracy=58.01: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 5, Avg Training Loss: 1.1535, Avg Time Taken = 360.57ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.1840, Accuracy: 5674/10000 (56.74%)\n","\n","Epoch 7/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.034713625907898 Batch_id=24 Accuracy=61.23: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 6, Avg Training Loss: 1.0800, Avg Time Taken = 357.37ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.1479, Accuracy: 5969/10000 (59.69%)\n","\n","Epoch 8/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9146243929862976 Batch_id=24 Accuracy=63.05: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 7, Avg Training Loss: 1.0241, Avg Time Taken = 362.02ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.0619, Accuracy: 6239/10000 (62.39%)\n","\n","Epoch 9/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9481847286224365 Batch_id=24 Accuracy=65.20: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 8, Avg Training Loss: 0.9718, Avg Time Taken = 362.40ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 1.0279, Accuracy: 6376/10000 (63.76%)\n","\n","Epoch 10/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9405885338783264 Batch_id=24 Accuracy=66.67: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 9, Avg Training Loss: 0.9324, Avg Time Taken = 360.19ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9473, Accuracy: 6653/10000 (66.53%)\n","\n","Epoch 11/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9161137342453003 Batch_id=24 Accuracy=68.36: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 10, Avg Training Loss: 0.8885, Avg Time Taken = 361.11ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9932, Accuracy: 6495/10000 (64.95%)\n","\n","Epoch 12/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8603850603103638 Batch_id=24 Accuracy=69.25: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 11, Avg Training Loss: 0.8613, Avg Time Taken = 361.51ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9441, Accuracy: 6640/10000 (66.40%)\n","\n","Epoch 13/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8587924838066101 Batch_id=24 Accuracy=70.73: 100%|██████████| 25/25 [00:17<00:00,  1.39it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 12, Avg Training Loss: 0.8264, Avg Time Taken = 362.89ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9430, Accuracy: 6648/10000 (66.48%)\n","\n","Epoch 14/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.874880313873291 Batch_id=24 Accuracy=71.69: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 13, Avg Training Loss: 0.7988, Avg Time Taken = 361.62ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8697, Accuracy: 6930/10000 (69.30%)\n","\n","Epoch 15/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8040109276771545 Batch_id=24 Accuracy=72.89: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 14, Avg Training Loss: 0.7657, Avg Time Taken = 359.15ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8769, Accuracy: 6948/10000 (69.48%)\n","\n","Epoch 16/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.7048029899597168 Batch_id=24 Accuracy=73.86: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 15, Avg Training Loss: 0.7370, Avg Time Taken = 362.69ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8686, Accuracy: 6936/10000 (69.36%)\n","\n","Epoch 17/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6863178014755249 Batch_id=24 Accuracy=74.64: 100%|██████████| 25/25 [00:17<00:00,  1.45it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 16, Avg Training Loss: 0.7148, Avg Time Taken = 360.20ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8452, Accuracy: 7060/10000 (70.60%)\n","\n","Epoch 18/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.674196720123291 Batch_id=24 Accuracy=75.55: 100%|██████████| 25/25 [00:17<00:00,  1.43it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 17, Avg Training Loss: 0.6898, Avg Time Taken = 358.37ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8681, Accuracy: 7003/10000 (70.03%)\n","\n","Epoch 19/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6903563737869263 Batch_id=24 Accuracy=76.16: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 18, Avg Training Loss: 0.6730, Avg Time Taken = 361.41ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8020, Accuracy: 7226/10000 (72.26%)\n","\n","---------- prev = 0.6898008227348328 current = 0.6729649662971496 ---------\n","Epoch 20/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6400915384292603 Batch_id=24 Accuracy=77.00: 100%|██████████| 25/25 [00:17<00:00,  1.46it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 19, Avg Training Loss: 0.6529, Avg Time Taken = 359.87ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7977, Accuracy: 7281/10000 (72.81%)\n","\n","Epoch 21/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6483575105667114 Batch_id=24 Accuracy=77.58: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 20, Avg Training Loss: 0.6334, Avg Time Taken = 359.12ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7772, Accuracy: 7288/10000 (72.88%)\n","\n","---------- prev = 0.6528990292549133 current = 0.6333779048919678 ---------\n","Epoch 22/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6078910231590271 Batch_id=24 Accuracy=78.23: 100%|██████████| 25/25 [00:17<00:00,  1.40it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 21, Avg Training Loss: 0.6116, Avg Time Taken = 360.48ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7825, Accuracy: 7298/10000 (72.98%)\n","\n","Epoch 23/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5993676781654358 Batch_id=24 Accuracy=79.22: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 22, Avg Training Loss: 0.5943, Avg Time Taken = 361.84ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7507, Accuracy: 7443/10000 (74.43%)\n","\n","---------- prev = 0.6115591692924499 current = 0.59430020570755 ---------\n","Epoch 24/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5777539014816284 Batch_id=24 Accuracy=79.86: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 23, Avg Training Loss: 0.5747, Avg Time Taken = 364.55ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7822, Accuracy: 7324/10000 (73.24%)\n","\n","---------- prev = 0.59430020570755 current = 0.5747307419776917 ---------\n","Epoch 25/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5954622030258179 Batch_id=24 Accuracy=80.24: 100%|██████████| 25/25 [00:17<00:00,  1.42it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 24, Avg Training Loss: 0.5650, Avg Time Taken = 359.70ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7520, Accuracy: 7468/10000 (74.68%)\n","\n","---------- prev = 0.5747307419776917 current = 0.5649992895126342 ---------\n","Epoch 26/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.5759264230728149 Batch_id=24 Accuracy=80.44: 100%|██████████| 25/25 [00:17<00:00,  1.44it/s]"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 25, Avg Training Loss: 0.5586, Avg Time Taken = 360.99ms\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7487, Accuracy: 7449/10000 (74.49%)\n","\n","---------- prev = 0.5649992895126342 current = 0.5586209177970887 ---------\n","Epoch 27/100\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.6129828691482544 Batch_id=24 Accuracy=80.87: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":[" --> EPOCH: 26, Avg Training Loss: 0.5443, Avg Time Taken = 361.21ms\n","\n","Test set: Average loss: 0.7521, Accuracy: 7460/10000 (74.60%)\n","\n","---------- prev = 0.5586209177970887 current = 0.5442671251296997 ---------\n","Model saved at: /content/drive/MyDrive/EPAi_V5/model_small_acc_80.pth\n","Early stopping triggered!\n"]}]},{"cell_type":"code","source":["model.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_small_acc_80.pth', weights_only=True))\n","test(model, device, test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IHwivY2wd0WI","executionInfo":{"status":"ok","timestamp":1737907430458,"user_tz":-330,"elapsed":2601,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"17aa022d-9757-4ead-da5e-7c3ab0657c61"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7521, Accuracy: 7460/10000 (74.60%)\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["0.7520527465820313"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["# Distillation Process"],"metadata":{"id":"6KbNLvnQgu3V"}},{"cell_type":"code","source":["class EarlyStopping:\n","    def __init__(self, tolerance=5, min_delta=0.01):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.prev_loss = None  # Initialize as None\n","        self.counter = 0\n","\n","    def __call__(self, train_loss):\n","        if self.prev_loss is None:  # First iteration\n","            self.prev_loss = train_loss\n","            return False  # Continue training\n","\n","        if (abs(train_loss - self.prev_loss)) < self.min_delta:\n","            print(f'---------- prev = {self.prev_loss} current = {train_loss} ---------')\n","            self.counter += 1\n","        else:\n","            self.counter = 0  # Reset counter if loss improves\n","\n","        self.prev_loss = train_loss\n","\n","        return self.counter >= self.tolerance  # Return True if stopping criteria met\n","\n","train_losses = []\n","test_losses = []\n","train_acc = []\n","test_acc = []\n","time_taken = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'Using device = {device}')\n","\n","# Prepare teacher model\n","teacher = TeacherModel().to(device)\n","teacher.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_heavy_acc_92.pth', weights_only=True))\n","teacher.eval()\n","\n","# Prepare student model\n","student = StudentModel().to(device)\n","student.load_state_dict(torch.load('/content/drive/MyDrive/EPAi_V5/model_small_acc_81.pth', weights_only=True))\n","\n","print(\"=============================================\")\n","print(\"Student model accuracy before training \")\n","student.eval()\n","test(student, device, test_loader)\n","print(\"=============================================\")\n","\n","# Loss functions\n","hard_loss = nn.CrossEntropyLoss() #Hard label loss\n","soft_loss = nn.KLDivLoss(reduction=\"batchmean\")  # Distillation loss\n","\n","# Temperature and alpha\n","T = 5.0  # Temperature\n","alpha = 0.5  # Weight for distillation loss\n","\n","# Optimizer\n","optimizer = optim.Adam(student.parameters(), lr=0.001)\n","\n","# Training loop\n","EPOCHS = 100\n","for epoch in range(EPOCHS):\n","\n","    correct = 0\n","    processed = 0\n","    epoch_loss = 0\n","    pbar = tqdm(train_loader)\n","    student.train()\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","\n","        optimizer.zero_grad()\n","\n","        # Teacher predictions (soft labels)\n","        with torch.no_grad():\n","            teacher_logits = teacher(data) / T\n","\n","        teacher_probs = torch.softmax(teacher_logits, dim=1)\n","\n","        # Student predictions\n","        student_logits = student(data)\n","        student_probs = torch.log_softmax(student_logits / T, dim=1)\n","\n","        # Compute losses\n","        loss_soft = soft_loss(student_probs, teacher_probs) * (T ** 2)  # Scale by T^2\n","        loss_hard = hard_loss(student_logits, target)\n","        loss = alpha * loss_hard + (1 - alpha) * loss_soft\n","\n","        epoch_loss += loss.item()\n","\n","        # Backpropagation\n","        loss.backward()\n","        optimizer.step()\n","\n","        pred = student_logits.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        correct += pred.eq(target.view_as(pred)).sum().item()\n","        processed += len(data)\n","\n","        pbar.set_description(desc=f'Loss={loss.item()} Batch_id={batch_idx} Accuracy={100 * correct / processed:0.2f}')\n","\n","    avg_train_loss = epoch_loss / len(train_loader)\n","    train_losses.append(avg_train_loss)\n","\n","    # Check for early stopping\n","    if early_stopping(avg_train_loss):\n","        try:\n","            # Ensure the directory exists\n","            save_dir = '/content/drive/MyDrive/EPAi_V5'\n","            os.makedirs(save_dir, exist_ok=True)\n","            PATH = os.path.join(save_dir, f'model_distil_acc_{int(train_acc[-1]):d}.pth')\n","        except OSError:\n","            # Fallback to current directory if Drive is unavailable\n","            PATH = f'./model_small_acc_{int(train_acc[-1]):d}.pth'\n","\n","        # Save the model weights\n","        torch.save(model.state_dict(), PATH)\n","        print(f\"Model saved at: {PATH}\")\n","        print(\"Early stopping triggered!\")\n","        break\n","\n","    student.eval()\n","\n","    test_loss = 0\n","    correct = 0\n","\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","\n","            output = student(data)\n","\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","    test_losses.append(test_loss)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"goXWg5nAgyrO","executionInfo":{"status":"error","timestamp":1737907796933,"user_tz":-330,"elapsed":358750,"user":{"displayName":"Aravind TSOI","userId":"00062020326449719419"}},"outputId":"cfc32c64-649a-4a7a-c9da-4c84f6b4dee3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device = cuda\n","=============================================\n","Student model accuracy before training \n","\n","Test set: Average loss: 0.7419, Accuracy: 7566/10000 (75.66%)\n","\n","=============================================\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.3082377910614014 Batch_id=24 Accuracy=76.71: 100%|██████████| 25/25 [00:18<00:00,  1.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.9304, Accuracy: 7478/10000 (74.78%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.2178696393966675 Batch_id=24 Accuracy=80.33: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7959, Accuracy: 7728/10000 (77.28%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0896053314208984 Batch_id=24 Accuracy=81.28: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7732, Accuracy: 7714/10000 (77.14%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0880683660507202 Batch_id=24 Accuracy=82.09: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7589, Accuracy: 7764/10000 (77.64%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0922045707702637 Batch_id=24 Accuracy=82.32: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.8024, Accuracy: 7725/10000 (77.25%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0072500705718994 Batch_id=24 Accuracy=82.76: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7968, Accuracy: 7740/10000 (77.40%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0438783168792725 Batch_id=24 Accuracy=83.13: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7820, Accuracy: 7802/10000 (78.02%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9212185740470886 Batch_id=24 Accuracy=83.24: 100%|██████████| 25/25 [00:18<00:00,  1.33it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.9830327153205871 current = 0.9655375790596008 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7392, Accuracy: 7823/10000 (78.23%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=1.0202991962432861 Batch_id=24 Accuracy=83.73: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.9655375790596008 current = 0.9466933465003967 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7113, Accuracy: 7915/10000 (79.15%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9273560047149658 Batch_id=24 Accuracy=84.27: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7360, Accuracy: 7865/10000 (78.65%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9515639543533325 Batch_id=24 Accuracy=84.56: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7050, Accuracy: 7949/10000 (79.49%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8512232899665833 Batch_id=24 Accuracy=85.07: 100%|██████████| 25/25 [00:18<00:00,  1.34it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7860, Accuracy: 7752/10000 (77.52%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8640667200088501 Batch_id=24 Accuracy=85.16: 100%|██████████| 25/25 [00:19<00:00,  1.29it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.8758802199363709 current = 0.8721289110183715 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7365, Accuracy: 7894/10000 (78.94%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8685557246208191 Batch_id=24 Accuracy=85.54: 100%|██████████| 25/25 [00:18<00:00,  1.36it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.8721289110183715 current = 0.86076979637146 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7165, Accuracy: 7938/10000 (79.38%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8674250841140747 Batch_id=24 Accuracy=85.67: 100%|██████████| 25/25 [00:19<00:00,  1.27it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.86076979637146 current = 0.8424052166938781 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.7170, Accuracy: 7957/10000 (79.57%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.9037202000617981 Batch_id=24 Accuracy=85.83: 100%|██████████| 25/25 [00:18<00:00,  1.38it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.8424052166938781 current = 0.8303352212905883 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.6974, Accuracy: 7936/10000 (79.36%)\n","\n"]},{"output_type":"stream","name":"stderr","text":["Loss=0.8141599297523499 Batch_id=24 Accuracy=86.05: 100%|██████████| 25/25 [00:18<00:00,  1.37it/s]"]},{"output_type":"stream","name":"stdout","text":["---------- prev = 0.8303352212905883 current = 0.8178793621063233 ---------\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"IndexError","evalue":"list index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-b9e55928c8d8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/EPAi_V5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'model_small_acc_{int(train_acc[-1]):d}.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# Fallback to current directory if Drive is unavailable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]}]}